{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain One-Hot Encoding**\n"
      ],
      "metadata": {
        "id": "WJcxFbm4fdiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Hot Encoding is a representation of categorical variables as numerical data, in which each category value is represented as a binary vector. Each vector has a length equal to the number of categories and a 1 in the position corresponding to the specific category and 0s in the other positions. It is used in natural language processing, computer vision and machine learning algorithms."
      ],
      "metadata": {
        "id": "B0maibLif17A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Explain Bag of Words**\n"
      ],
      "metadata": {
        "id": "TYDMNeo1fmdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words (BoW) is a method used in NLP to represent the occurrence of words in a document as a vector. It considers each document as a bag of words, ignoring the order of the words, and assigns a frequency count to each word. The resulting vector representation of a document can then be used as input to a machine learning algorithm."
      ],
      "metadata": {
        "id": "24TW-iY_f50P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Explain Bag of N-Grams**\n"
      ],
      "metadata": {
        "id": "WWPNhLeNfqZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of N-Grams (BoN) is an extension of the BoW representation, where instead of considering individual words, contiguous sequences of N words (N-grams) are used to represent a document. N-grams can capture partial information about the word order in a document and can be useful for certain tasks such as sentiment analysis."
      ],
      "metadata": {
        "id": "8JheMtxaf-H9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Explain TF-IDF**"
      ],
      "metadata": {
        "id": "czXEpFmpfvH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic used to represent the importance of a word in a document or a collection of documents. It is a product of two metrics: term frequency (TF), which measures the number of times a word appears in a document, and inverse document frequency (IDF), which measures the rarity of the word across the collection of documents. TF-IDF assigns higher weights to words that are more frequent in a specific document, but less frequent across the collection, thus reflecting their importance in the document's content.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_ri5BrzYgBP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. What is OOV problem?**\n"
      ],
      "metadata": {
        "id": "Fa6MjExigGWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OOV (Out-Of-Vocabulary) problem refers to the situation where a model encounters a word during testing that was not present in its training data. This can cause difficulties for models such as Bag of Words or TF-IDF that rely on a fixed vocabulary and require an exact match between the words in the training data and the test data."
      ],
      "metadata": {
        "id": "oTU8G2o7gUid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. What are word embeddings?**\n"
      ],
      "metadata": {
        "id": "kYmzNMP7gJd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Embeddings are a type of representation for text data where each word is represented by a dense vector of real numbers. Unlike One-Hot Encoding or Bag of Words, word embeddings capture semantic relationships between words and their meanings. They are learned by a neural network during the training process and can be used as inputs to other models, such as neural networks or other machine learning algorithms."
      ],
      "metadata": {
        "id": "HJJP2AR7gXsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Explain Continuous bag of words (CBOW)**\n"
      ],
      "metadata": {
        "id": "Te44_yeNgMar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous Bag of Words (CBOW) is a method used to train word embeddings, specifically in the context of NLP. The CBOW model uses the context words surrounding a target word to predict the target word. During training, the model learns to associate the context words with the target word and generates word embeddings that reflect their relationship."
      ],
      "metadata": {
        "id": "oa0d17E-gbLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Explain SkipGram**"
      ],
      "metadata": {
        "id": "JgO5AtUJgQKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SkipGram is a method used to train word embeddings, similar to CBOW. Unlike CBOW, the SkipGram model uses the target word to predict the surrounding context words. The model learns to associate the target word with its surrounding context words and generates word embeddings that reflect their relationship.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pg9GrUOSgecR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Explain Glove Embeddings.**"
      ],
      "metadata": {
        "id": "isuGFhzUgkja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GLoVe (Global Vectors for Word Representation) is a type of word embedding method that is trained on co-occurrence statistics of words in a corpus. GLoVe captures the semantic relationships between words by modeling the co-occurrence probabilities of words in a corpus and representing each word as a dense vector of real numbers. The resulting word embeddings are learned from the corpus and can capture relationships such as word analogies and similarities. GLoVe has been shown to perform well on various NLP tasks and is widely used in industry and academia."
      ],
      "metadata": {
        "id": "UCebtLfXgpo3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUJu33DTfP2_"
      },
      "outputs": [],
      "source": []
    }
  ]
}