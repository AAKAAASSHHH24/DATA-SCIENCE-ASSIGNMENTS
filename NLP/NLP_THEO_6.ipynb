{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What are Vanilla autoencoders**\n"
      ],
      "metadata": {
        "id": "O0JWcHZNqAHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanilla autoencoders are the simplest form of autoencoders, where the goal is to learn a compact representation (encoding) of the input data that can be used to reconstruct the original input. A vanilla autoencoder consists of two parts: an encoder that compresses the input data into a lower-dimensional representation, and a decoder that takes the compressed representation and generates an approximation of the original input."
      ],
      "metadata": {
        "id": "tmGo-bSnqS36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. What are Sparse autoencoders**\n"
      ],
      "metadata": {
        "id": "OX4FocbgqHiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sparse autoencoders are a variation of the vanilla autoencoder that add a sparsity constraint to the encoding to encourage the model to learn only the most important features of the input data. The sparsity constraint is typically achieved by adding a regularization term to the loss function that penalizes the number of active neurons in the encoding."
      ],
      "metadata": {
        "id": "4QbyI9z8qWRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What are Denoising autoencoders**\n"
      ],
      "metadata": {
        "id": "pwAOinWFqLhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Denoising autoencoders are a type of autoencoder that are trained to reconstruct a clean version of the input from a corrupted version of the input. The goal is to learn a robust representation of the input that can recover the original signal even when it is contaminated with noise. This type of autoencoder can be used for image denoising, anomaly detection, and other applications where the goal is to recover the original signal from a degraded version of the signal."
      ],
      "metadata": {
        "id": "BwBjs5haqZmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What are Convolutional autoencoders**"
      ],
      "metadata": {
        "id": "myOOjWSwqOxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional autoencoders are a type of autoencoder that use convolutional layers instead of fully connected layers to learn the encoding and decoding functions. Convolutional autoencoders are particularly well-suited for image and video data, as the convolutional layers can capture spatial relationships between pixels in the image. Convolutional autoencoders can be used for tasks such as image compression, image denoising, and image generation."
      ],
      "metadata": {
        "id": "dOvAxYCiqcts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. What are Stacked autoencoders**\n"
      ],
      "metadata": {
        "id": "UaXtPNsRqklF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stacked Autoencoders are a type of deep learning architecture where multiple autoencoder models are stacked together to form a single deep neural network. This enables the autoencoder to learn higher-level representations of the input data and results in better performance compared to a single autoencoder."
      ],
      "metadata": {
        "id": "Voj2v1ekqyLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Explain how to generate sentences using LSTM autoencoders**\n"
      ],
      "metadata": {
        "id": "n5tqrRLZqoLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate sentences using LSTM autoencoders, one can first train the LSTM autoencoder on a large corpus of text data to learn the patterns and structure of the language. Then, during the generation phase, the autoencoder can be used to generate new sentences by starting with a random seed word and then generating the next word in the sentence based on the hidden state of the LSTM and the probability distribution over the vocabulary."
      ],
      "metadata": {
        "id": "D1lCLZTEq2Wh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Explain Extractive summarization**\n"
      ],
      "metadata": {
        "id": "9iPP9umiqrTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extractive summarization is a method for summarizing text by selecting the most relevant sentences or phrases from the original document to form a new, condensed version. This approach typically involves scoring the sentences based on metrics such as term frequency, term frequency-inverse document frequency (TF-IDF), or cosine similarity, and then selecting the top-scoring sentences to include in the summary."
      ],
      "metadata": {
        "id": "9YF2DgH-q51k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Explain Abstractive summarization**"
      ],
      "metadata": {
        "id": "d8B9kjesquGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abstractive summarization is a method for summarizing text by generating a new, condensed version that is not necessarily a verbatim copy of the original document. This approach typically involves using advanced natural language processing techniques such as deep learning models, such as encoder-decoder LSTMs, to generate a summary that captures the key points and meaning of the original text in a concise and coherent manner."
      ],
      "metadata": {
        "id": "f2lcyDHBq861"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Explain Beam search**\n"
      ],
      "metadata": {
        "id": "MAev76qXrCgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beam search is a best-first search algorithm that explores the search space by expanding the most promising node. In NLP tasks, beam search is often used in decoding tasks such as machine translation and text summarization, to produce the output sequence with the highest probability by considering the most likely candidate at each decoding step."
      ],
      "metadata": {
        "id": "gnxBDPaUrPtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Explain Length normalization**\n"
      ],
      "metadata": {
        "id": "2F6X06Y1rFi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Length normalization is a technique used to adjust the score of a candidate sequence generated by beam search, to account for its length. The goal is to penalize longer sequences and to balance the trade-off between the quality and the length of the output."
      ],
      "metadata": {
        "id": "PhS6lO-_rS7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Explain Coverage normalization**\n"
      ],
      "metadata": {
        "id": "BLAXDliurI5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coverage normalization is a technique used in beam search to prevent repeated words in the generated output. The idea is to keep track of the words that have already been generated, and to penalize the beam search scores of candidate sequences that include already generated words."
      ],
      "metadata": {
        "id": "AiYEj-barWCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Explain ROUGE metric evaluation**"
      ],
      "metadata": {
        "id": "UBI2sHtSrLuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric is a popular evaluation metric for text summarization. It compares the generated summary against a reference summary, and measures the recall rate of the n-grams (unigrams, bigrams, trigrams, etc.) in the reference summary that also appear in the generated summary. There are different variants of the ROUGE metric, including ROUGE-N, ROUGE-L, and ROUGE-W, each with a slightly different calculation method."
      ],
      "metadata": {
        "id": "SjyiAluqrabv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IL19vh-p-8F"
      },
      "outputs": [],
      "source": []
    }
  ]
}