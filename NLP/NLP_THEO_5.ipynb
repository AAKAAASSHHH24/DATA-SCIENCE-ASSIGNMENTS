{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What are Sequence-to-sequence models?**\n"
      ],
      "metadata": {
        "id": "4st1yIkhoGLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequence-to-sequence models:\n",
        "\n",
        "Sequence-to-sequence (Seq2Seq) models are neural network models that are used to map input sequences to output sequences. They are widely used in natural language processing, speech recognition, and machine translation tasks. Seq2Seq models consist of an encoder that takes in the input sequence and compresses it into a fixed-length vector representation, and a decoder that takes in the vector representation and generates the corresponding output sequence."
      ],
      "metadata": {
        "id": "xmauC2wrodsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. What are the Problem with Vanilla RNNs?**\n"
      ],
      "metadata": {
        "id": "MxF9GhbuoTeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problems with Vanilla RNNs:\n",
        "\n",
        "Vanishing Gradient Problem: The gradients of the parameters in the RNNs can become very small over many time steps, making it difficult for the model to learn long-term dependencies in the data.\n",
        "Exploding Gradient Problem: The gradients can become very large and cause the model to over-fit or diverge, making it difficult to train the model.\n"
      ],
      "metadata": {
        "id": "lkBLmO6BoiPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What is Gradient clipping?**\n"
      ],
      "metadata": {
        "id": "nS86oBoloWfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Clipping:\n",
        "\n",
        "Gradient clipping is a technique used to address the exploding gradient problem in RNNs and other deep neural networks. It involves clipping the gradients to a fixed range, such as [-1, 1], to prevent them from growing too large and causing the model to diverge during training. This helps stabilize the training process and improves convergence."
      ],
      "metadata": {
        "id": "vUpnvdfMomRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Explain Attention mechanism**"
      ],
      "metadata": {
        "id": "AmJr8NJSoZXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention Mechanism:\n",
        "\n",
        "An attention mechanism is a mechanism in deep learning models that allows the model to focus on different parts of the input sequence at different times. The attention mechanism computes a weight for each time step in the input sequence, indicating the importance of each time step for generating the final output. The attention weights are used to weigh the input representation when computing the final output. This allows the model to dynamically focus on different parts of the input, improving the ability of the model to capture long-term dependencies and handle input sequences of varying lengths.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cSL0UNUooqgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Explain Conditional random fields (CRFs)**\n"
      ],
      "metadata": {
        "id": "p3r8Mmh1ozwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional Random Fields (CRFs):\n",
        "\n",
        "Conditional Random Fields (CRFs) are a type of probabilistic graphical model used for structured prediction tasks. CRFs are commonly used in sequence labeling problems such as Named Entity Recognition (NER) and Part-of-Speech (POS) tagging, where the goal is to predict the labels for each element in a sequence given the context of the sequence. CRFs model the probability distribution over the sequence labels, given the input sequence, by defining a set of potentials or scores that capture the dependencies between the labels. The optimal label sequence is then selected by maximizing the conditional probability of the labels given the input sequence."
      ],
      "metadata": {
        "id": "6aJnwc8Ho_6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Explain self-attention**\n"
      ],
      "metadata": {
        "id": "lWvRDLQwo3w5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-Attention:\n",
        "\n",
        "Self-attention is a type of attention mechanism used in neural networks, particularly in the Transformer architecture. Self-attention allows each time step in a sequence to attend to all other time steps in the sequence to compute a representation of the entire sequence. Self-attention enables the model to capture long-term dependencies in the sequence without having to rely on recurrence or convolution, making it well-suited for processing sequences of varying lengths."
      ],
      "metadata": {
        "id": "MxMkt-h9pFOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. What is Bahdanau Attention?**"
      ],
      "metadata": {
        "id": "VXnQFhKLo6Zk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bahdanau Attention:\n",
        "\n",
        "Bahdanau Attention is a type of attention mechanism used in encoder-decoder RNNs. It is named after its inventor, Dzmitry Bahdanau, who proposed it in 2014. Bahdanau Attention is a type of content-based attention, where the attention mechanism is used to focus on specific parts of the input sequence when generating the output sequence. The attention mechanism is trained end-to-end and uses a feed-forward neural network to compute the attention scores, which are used to weigh the input representation when computing the final output. Bahdanau Attention has been used in many successful NLP models, including machine translation, abstractive summarization, and text-to-speech synthesis."
      ],
      "metadata": {
        "id": "9Iqb2f8-pIvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. What is a Language Model?**\n"
      ],
      "metadata": {
        "id": "3lzSq2f-pOOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Language Model:\n",
        "\n",
        "A language model is a probabilistic model that is trained to predict the likelihood of a sequence of words in a language. The goal of a language model is to assign high probabilities to sequences that are likely to occur in the language, and low probabilities to sequences that are unlikely to occur. Language models are used in many NLP tasks, including text generation, machine translation, and speech recognition."
      ],
      "metadata": {
        "id": "BM-qIlNHpYaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. What is Multi-Head Attention?**\n"
      ],
      "metadata": {
        "id": "zX29uRPLpRSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Head Attention:\n",
        "\n",
        "Multi-Head Attention is an extension of the attention mechanism that allows a model to attend to multiple aspects of the input sequence simultaneously. In Multi-Head Attention, the attention mechanism is applied multiple times, with each head attending to a different aspect of the input. The results from each head are concatenated and combined to form the final representation, allowing the model to capture different types of relationships between elements in the input sequence. Multi-Head Attention is widely used in the Transformer architecture and has been shown to be effective in many NLP tasks."
      ],
      "metadata": {
        "id": "pp_DJhSspcuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. What is Bilingual Evaluation Understudy (BLEU)**"
      ],
      "metadata": {
        "id": "l1G2TY2JpUMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bilingual Evaluation Understudy (BLEU):\n",
        "Bilingual Evaluation Understudy (BLEU) is a metric used to evaluate the performance of machine translation systems. The BLEU score measures the degree of similarity between a candidate translation and a reference translation, usually expressed as the number of overlapping n-grams (e.g. 1-grams, 2-grams, etc.) between the two. The BLEU score is widely used in the machine translation community as it provides a fast and simple way to compare different systems and evaluate the quality of their translations. The BLEU score is typically used as an objective measure in machine translation research and development, and is often used in conjunction with human evaluations to provide a more complete picture of system performance."
      ],
      "metadata": {
        "id": "JjAgdZduphAP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImPLqiYan7v7"
      },
      "outputs": [],
      "source": []
    }
  ]
}