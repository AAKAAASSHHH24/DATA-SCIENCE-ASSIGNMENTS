{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What are Corpora?**\n"
      ],
      "metadata": {
        "id": "wpR7ON4XhmYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Corpora are collections of text data that are used for natural language processing tasks. They can range from a single document to a large corpus of multiple documents and can be used for tasks such as text classification, information retrieval, or machine translation.\n",
        "\n"
      ],
      "metadata": {
        "id": "0mR4IsyqhxsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. What are Tokens?**\n"
      ],
      "metadata": {
        "id": "uaBRrD-hhp7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokens are the basic building blocks of NLP and refer to the individual elements of a text after it has been pre-processed and split into its constituent parts. Tokens can be words, punctuation marks, numbers, or any other meaningful units of a text."
      ],
      "metadata": {
        "id": "i32mxqxZh1a0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What are Unigrams, Bigrams, Trigrams?**"
      ],
      "metadata": {
        "id": "s8px-NkDhtcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unigrams, Bigrams, and Trigrams are N-grams, which are contiguous sequences of N items (in this case words) in a text. Unigrams are individual words, Bigrams are two-word sequences, and Trigrams are three-word sequences. N-grams are commonly used in NLP to capture partial information about the word order in a text, and they play a crucial role in tasks such as text classification and language modeling.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5h5VIziah4YF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. How to generate n-grams from text?**\n"
      ],
      "metadata": {
        "id": "kARmjlwjh_N3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating N-grams from text involves splitting a text into its constituent words and then forming contiguous sequences of N words. This can be done using string manipulations, regular expressions, or specific NLP libraries. For example, in Python, the NLTK library provides functions for generating N-grams from text."
      ],
      "metadata": {
        "id": "ZwBjXnqeiJs8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Explain Lemmatization**\n"
      ],
      "metadata": {
        "id": "OVM-IfEviCSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is a text normalization technique that reduces words to their base form, or lemma. The goal of lemmatization is to reduce words to their base form, so that words with the same lemma can be treated as the same word. For example, the words \"running,\" \"ran,\" and \"run\" would all be reduced to the lemma \"run.\" Lemmatization is different from stemming, as it considers the context and uses a dictionary-based approach to determine the lemma of a word."
      ],
      "metadata": {
        "id": "DiruDxAmiNqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Explain Stemming**"
      ],
      "metadata": {
        "id": "e9AInA2WiFAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is a text normalization technique that reduces words to their root form, or stem. The goal of stemming is to reduce words to their root form, so that words with the same root form can be treated as the same word. For example, the words \"running,\" \"ran,\" and \"run\" would all be reduced to the stem \"run.\" Stemming is a more aggressive approach than lemmatization, as it uses heuristics and rules to determine the root form of a word, without considering the context."
      ],
      "metadata": {
        "id": "akc679lbiRCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Explain Part-of-speech (POS) tagging**\n"
      ],
      "metadata": {
        "id": "TEOm4zcIiWcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part-of-speech (POS) tagging is a technique in NLP that involves assigning a grammatical category (such as noun, verb, adjective, adverb, etc.) to each word in a text. This is useful for tasks such as syntactic parsing and text classification, as the part-of-speech of a word can provide important information about its role in a sentence. POS tagging is performed using NLP tools, such as rule-based systems or statistical models, and can be used as a pre-processing step for other NLP tasks."
      ],
      "metadata": {
        "id": "uNbbsA_7ilED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Explain Chunking or shallow parsing**\n"
      ],
      "metadata": {
        "id": "Ah9gvqjGiZgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking, also known as shallow parsing, is a technique in NLP that involves identifying and extracting smaller, non-overlapping segments of text (chunks) that represent higher-level syntactic structure. Chunks can be phrases, clauses, or any other syntactically meaningful units. Chunking is performed using NLP tools, such as rule-based systems or statistical models, and can provide a representation of the syntactic structure of a sentence that is more meaningful than individual words or POS tags."
      ],
      "metadata": {
        "id": "JPJVh3h6ioP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Explain Noun Phrase (NP) chunking**\n"
      ],
      "metadata": {
        "id": "OdnFZsdUidAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noun Phrase (NP) chunking is a specific type of chunking that involves extracting noun phrases from a sentence. Noun phrases typically consist of a noun and its dependents, such as determiners, adjectives, or prepositional phrases. NP chunking is used in NLP for tasks such as named entity recognition, event extraction, or text classification, as noun phrases can provide important information about the entities and events mentioned in a text."
      ],
      "metadata": {
        "id": "7mmR-MSxirkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Explain Named Entity Recognition**"
      ],
      "metadata": {
        "id": "JPYTpun9igol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition (NER) is a subtask of NLP that involves identifying and extracting entities such as person names, organizations, locations, and dates from a text. NER is an important pre-processing step for tasks such as information extraction, event extraction, or text classification, as entities can provide important information about the content of a text. NER is performed using NLP tools, such as rule-based systems or statistical models, and can be combined with other NLP techniques such as NP chunking or POS tagging to improve its performance."
      ],
      "metadata": {
        "id": "IF0riYrriuO6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GE93X2ghed2"
      },
      "outputs": [],
      "source": []
    }
  ]
}