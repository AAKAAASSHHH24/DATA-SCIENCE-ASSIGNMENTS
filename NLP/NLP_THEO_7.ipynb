{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain the architecture of BERT**"
      ],
      "metadata": {
        "id": "pM1oNWHwsRfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based architecture designed for pre-training contextual representations from a large corpus of text. It is pre-trained on a large corpus of text and can then be fine-tuned for specific NLP tasks.\n",
        "\n",
        "The basic architecture of BERT consists of the following components:\n",
        "\n",
        "Embedding layer: This layer maps the input token indices to the corresponding dense vectors (word embeddings).\n",
        "\n",
        "Encoder layer: This is the main component of the BERT architecture. It is composed of multiple Transformer blocks, each consisting of multi-head self-attention and fully connected feedforward layers. The Transformer blocks are used to process the input sequence in a parallel manner, allowing BERT to learn context-aware representations of the input tokens.\n",
        "\n",
        "Pooler layer: This layer is responsible for generating a fixed-length representation (vector) of the input sequence. It is typically a fully connected layer that is applied to the output of the last Transformer block.\n",
        "\n",
        "Masked language modeling (MLM) head: This component is used during pre-training, where BERT is trained to predict the masked tokens in the input sequence. The MLM head takes the output from the encoder and generates predictions for the masked tokens based on the context in the surrounding sequence.\n",
        "\n",
        "Next sentence prediction (NSP) head: This component is also used during pre-training, where BERT is trained to predict if the two sentences in the input sequence are contiguous or not.\n",
        "\n",
        "In summary, BERT is an encoder-based architecture that uses self-attention and fully connected layers to process the input sequence in a parallel manner. The output of the last layer is a fixed-length representation of the input sequence, which can be fine-tuned for specific NLP tasks.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hDCd1zvosLfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What is Matthews evaluation?**\n"
      ],
      "metadata": {
        "id": "1A-6xx0gsq-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. What is Matthews Correlation Coefficient (MCC)?**"
      ],
      "metadata": {
        "id": "M7KZaNQasuRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Matthews Correlation Coefficient (MCC) is a measure of the quality of binary (two-class) classifications. It is used to evaluate the performance of a binary classifier system, by comparing the actual and predicted classifications. MCC is a normalized measure of the correlation between the predicted and actual binary classifications, and is a value between -1 and 1. A value of 1 indicates perfect agreement between the predicted and actual classifications, while a value of -1 indicates total disagreement. A value of 0 indicates random prediction. The MCC is a useful metric when the class distribution is imbalanced or the cost of false positive and false negative predictions is different."
      ],
      "metadata": {
        "id": "_H4SJ2Ies1Om"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Why Fine-tuning a BERT model takes less time than pretraining**"
      ],
      "metadata": {
        "id": "ryc4qMSXs82b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning a pre-trained BERT model takes less time than pretraining because the model has already learned the general language representations from a large corpus of text during the pretraining phase. When fine-tuning, the model only needs to adapt these representations to the specific task at hand, which typically involves updating a small number of parameters relative to the entire model. This fine-tuning process is faster than pretraining because the model has already learned the general representations that are relevant to the task, and therefore only needs to make minor modifications to fit the specific task. Additionally, the fine-tuning process can benefit from transfer learning, allowing the model to leverage its prior knowledge to achieve good performance with fewer training examples and shorter training times."
      ],
      "metadata": {
        "id": "WTxAjcBptCIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Recognizing Textual Entailment (RTE)**\n"
      ],
      "metadata": {
        "id": "ew7VHe1xtJDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recognizing Textual Entailment (RTE) is a task in NLP that involves determining the relationship between two texts, usually a premise and a hypothesis. The task is to classify whether the hypothesis can be inferred from the premise."
      ],
      "metadata": {
        "id": "qUFwaRKKtSt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Explain the decoder stack of GPT models.**"
      ],
      "metadata": {
        "id": "s-2eML39tMPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder stack of GPT models refers to the part of the model that processes and generates the output sequence. GPT models are autoregressive language models, meaning they generate the next token in a sequence based on the previous tokens. The decoder stack of GPT models typically consists of a series of fully connected layers, and sometimes includes additional self-attention mechanisms to improve the model's ability to process context information. The output of the decoder is fed back into the input at each time step to allow the model to generate the next token in the sequence."
      ],
      "metadata": {
        "id": "5zN3ehMHtXlT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLG5rJCzr1OR"
      },
      "outputs": [],
      "source": []
    }
  ]
}