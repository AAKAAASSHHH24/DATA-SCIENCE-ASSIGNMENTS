{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**3. Name three advantages of the ELU activation function over ReLU.**\n"
      ],
      "metadata": {
        "id": "C6PDG_kv6rF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three advantages of the ELU activation function over ReLU:\n",
        "ELU has a mean activation closer to zero, which helps to alleviate the vanishing gradient problem during training deep neural networks.\n",
        "ELU has negative values that can serve as an \"excitement\" signal, encouraging the activation of different neurons, providing a richer representation.\n",
        "ELU has a smooth derivative, which can lead to faster convergence compared to ReLU and its variants."
      ],
      "metadata": {
        "id": "pEjiNy2Q61lO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. In which cases would you want to use each of the following activation functions: ELU, leaky\n",
        "ReLU (and its variants), ReLU, tanh, logistic, and softmax?**"
      ],
      "metadata": {
        "id": "BpYWkTIk6wKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation functions selection depends on the problem being solved and the architecture being used. Here are some general guidelines:\n",
        "ELU: Use it when you want a network that has a mean activation closer to zero, and when the vanishing gradient problem is a concern.\n",
        "Leaky ReLU: Use it when you have a lot of negative inputs and want to mitigate the \"dying ReLU\" problem (when a neuron stops learning)\n",
        "ReLU: Use it when you want a simple and computationally efficient activation function, and when the vanishing gradient problem is not a concern.\n",
        "tanh: Use it when you want an activation function that maps input to the range of -1 to 1.\n",
        "logistic: Use it when you want a smooth probability distribution, especially when you are working with binary classification problems.\n",
        "softmax: Use it when you want to predict a multiclass classification problem, and want to convert the outputs into probability distributions."
      ],
      "metadata": {
        "id": "4QWXs8ea64-M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBhV4crm6RV5"
      },
      "outputs": [],
      "source": []
    }
  ]
}