{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. How does unsqueeze help us to solve certain broadcasting problems?**\n"
      ],
      "metadata": {
        "id": "Fe44v-rfszmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The unsqueeze method in PyTorch allows us to add a singleton dimension to a tensor, making it possible to broadcast along that dimension. This can be useful in certain operations where the shape of the input tensors do not match but can be made to match with the help of unsqueeze."
      ],
      "metadata": {
        "id": "_-qIrGUPtBir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. How can we use indexing to do the same operation as unsqueeze?**\n"
      ],
      "metadata": {
        "id": "ZxL6M1DBs5na"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing can be used to achieve similar results to unsqueeze by adding a new dimension to the tensor. For example, tensor[None, :] would add a new dimension at the start of the tensor."
      ],
      "metadata": {
        "id": "hCHfF3R-tGNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. How do we show the actual contents of the memory used for a tensor?**"
      ],
      "metadata": {
        "id": "pRqPEd9qs9TW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, you can access the underlying data of a tensor by using the .data or .numpy() method. The .data method returns a tensor, while the .numpy() method returns a Numpy array that can be used to inspect the contents of the memory used by the tensor. For example, print(tensor.numpy()) would print the contents of the memory used by the tensor.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SCVFNhZbtJ2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added\n",
        "to each row or each column of the matrix? (Be sure to check your answer by running this\n",
        "code in a notebook.)**\n"
      ],
      "metadata": {
        "id": "5TQRaZFTuBsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When adding a vector of size 3 to a matrix of size 3x3, the elements of the vector are added to each row of the matrix. This is a broadcast operation, where the vector is \"stretched\" along the columns dimension to match the shape of the matrix. You can check this by running the following code in a notebook:"
      ],
      "metadata": {
        "id": "o2Tpz1_buUeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "vector = torch.tensor([1, 2, 3])\n",
        "matrix = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "result = matrix + vector\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyMEmUaYuXyx",
        "outputId": "0a44e977-bfc7-4ec9-bfbe-dcc36570cfcc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2,  4,  6],\n",
            "        [ 5,  7,  9],\n",
            "        [ 8, 10, 12]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Do broadcasting and expand_as result in increased memory use? Why or why not?**\n"
      ],
      "metadata": {
        "id": "Euv_xLyHuJgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Broadcasting and expand_as can result in increased memory usage, but this is not always the case. It depends on the specific operation being performed and the memory usage of the input tensors. For example, if the input tensors are already stored in contiguous blocks of memory, then a broadcast operation can be performed without increasing memory usage. On the other hand, if the input tensors are not contiguous, then the broadcast operation may result in the creation of new memory to store the result."
      ],
      "metadata": {
        "id": "_5WzocIUug1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Implement matmul using Einstein summation.**"
      ],
      "metadata": {
        "id": "V0PzxPzBuQMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The matmul operation can be implemented using Einstein summation as follows:"
      ],
      "metadata": {
        "id": "lP_okieFulzf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t5lUm6hqrbU-"
      },
      "outputs": [],
      "source": [
        "def matmul(mat1, mat2):\n",
        "    result = torch.einsum(\"ij, jk -> ik\", mat1, mat2)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This implementation uses the einsum function from PyTorch, which allows you to perform Einstein summation operations on tensors. The first argument to einsum is a string that specifies the indices being contracted and the output indices. The second and third arguments are the input tensors. In this case, we are contracting the indices i and j of mat1 and mat2 and creating the output index i on the result tensor."
      ],
      "metadata": {
        "id": "eVSkb2UVusaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. What are the forward pass and backward pass of a neural network?**\n"
      ],
      "metadata": {
        "id": "CMZv9DS0xLQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The forward pass and backward pass in a neural network refer to the two stages of the training process for a neural network. The forward pass involves passing input data through the network, calculating the activations of each layer and using them to produce an output. The backward pass involves using the gradients of the loss with respect to the network's output to calculate the gradients of the loss with respect to the network's parameters. This allows us to update the parameters in a way that reduces the loss."
      ],
      "metadata": {
        "id": "FxNN2aOpxbfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Why do we need to store some of the activations calculated for intermediate layers in the\n",
        "forward pass?**\n"
      ],
      "metadata": {
        "id": "NOyKmcqaxO4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the forward pass of a neural network, we need to store some of the activations of the intermediate layers so that we can use them in the backward pass to calculate the gradients of the loss with respect to the parameters. This is necessary because the gradients are calculated by working backwards through the network, starting with the output layer and ending with the input layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "aCNiuSg9xexy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. What is the downside of having activations with a standard deviation too far away from 1?**\n"
      ],
      "metadata": {
        "id": "MA2Ui-07xTec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activations with a standard deviation too far away from 1 can lead to the vanishing or exploding gradient problem. This occurs because the gradients of the loss with respect to the activations will be either very small or very large, making it difficult for the gradients to propagate effectively through the network during the backward pass. This can result in slow convergence or failure to converge altogether.\n",
        "\n"
      ],
      "metadata": {
        "id": "noWFKDTOxh1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. How can weight initialization help avoid this problem?**"
      ],
      "metadata": {
        "id": "BC9yzjP1xW-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight initialization plays an important role in avoiding the vanishing or exploding gradient problem. A good weight initialization strategy ensures that the activations have a standard deviation close to 1, making it easier for the gradients to propagate effectively through the network. This can be achieved by initializing the weights with small random values, or by using more sophisticated methods such as Glorot or He weight initialization."
      ],
      "metadata": {
        "id": "296tN5dixlKp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-D7ys3yJuopT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}