{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What are the key tasks involved in getting ready to work with machine learning modeling?**"
      ],
      "metadata": {
        "id": "GfuaK3q1mfVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key tasks involved in getting ready to work with machine learning modeling include:\n",
        "\n",
        "# **Defining the problem:** \n",
        "\n",
        "The first step in working with machine learning is to clearly define the problem you are trying to solve. This will involve identifying the goals of the model and the desired outcomes.\n",
        "\n",
        "# **Collecting and preparing the data:**\n",
        "\n",
        " The next step is to collect and prepare the data that will be used to train and evaluate the model. This may involve sourcing data from multiple sources, cleaning and preprocessing the data, and spliting the data into training and test sets.\n",
        "\n",
        "# **Choosing an appropriate evaluation metric:** \n",
        "\n",
        "It is important to choose an evaluation metric that is appropriate for the problem you are trying to solve. The chosen metric will be used to evaluate the performance of the model on the test set.\n",
        "\n",
        "# **Selecting a model:**\n",
        "\n",
        " Once the data is prepared, the next step is to select a machine learning model that is appropriate for the problem. There are many different types of models to choose from, and the best choice will depend on the characteristics of the data and the problem.\n",
        "\n",
        "# **Training and evaluating the model:**\n",
        "\n",
        " After selecting a model, the next step is to train it on the training data and evaluate its performance on the test data using the chosen evaluation metric.\n",
        "\n",
        "# **Fine-tuning the model:**\n",
        "\n",
        " If the model's performance is not satisfactory, you may need to fine-tune the model by adjusting its hyperparameters or selecting a different model.\n",
        "\n",
        "# **Deploying the model:** \n",
        "\n",
        "Once the model is performing well on the test set, it is ready to be deployed in the real world. This may involve integrating the model into an existing system or creating a new system to utilize the model's predictions."
      ],
      "metadata": {
        "id": "3Q6umLvhm-PP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Distinguish\n",
        "Feature selection vs. dimensionality reduction**"
      ],
      "metadata": {
        "id": "ejBaOOEOoMxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection and dimensionality reduction are two techniques that are often used in machine learning to reduce the number of features in a dataset and improve model performance.\n",
        "\n",
        "Feature selection is the process of selecting a subset of the most relevant features from a dataset to use in building a machine learning model. The goal of feature selection is to select a subset of features that have the highest predictive power and are most relevant to the problem being solved.\n",
        "\n",
        "Dimensionality reduction, on the other hand, is the process of reducing the number of features in a dataset by creating new, derived features that are combinations of the original features. Dimensionality reduction techniques, such as principal component analysis (PCA), can be used to identify patterns in the data and create a smaller number of new features that capture the most important information from the original features.\n",
        "\n",
        "In general, feature selection is useful when the number of features in the dataset is large and there are many irrelevant or redundant features that are not useful for the model. Dimensionality reduction is useful when the number of features is large and there may be patterns in the data that can be captured using a smaller number of derived features."
      ],
      "metadata": {
        "id": "D4yx7jnAoizN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Make quick notes on any two of the following:\n",
        "PCA**"
      ],
      "metadata": {
        "id": "HMTym5auqXCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal component analysis (PCA) is a dimensionality reduction technique that is commonly used in machine learning. It is a technique that is used to reduce the number of features in a dataset by **creating new, derived features that are combinations of the original features.**\n",
        "\n",
        "The goal of PCA is to identify patterns in the data and create a smaller number of new features that capture the most important information from the original features. These new features, called principal components, are created in such a way that the first principal component captures the maximum amount of variance in the data, the second principal component captures the second-highest amount of variance, and so on.\n",
        "\n",
        "PCA is a useful technique when the number of features in a dataset is large and there may be patterns in the data that can be captured using a smaller number of derived features. It can also be used to reduce the computational cost of training a machine learning model, as training a model with fewer features is usually faster and requires less memory."
      ],
      "metadata": {
        "id": "9lSGtaVGqfMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Why is it necessary to investigate data? Is there a discrepancy in how qualitative and quantitative\n",
        "data are explored?**"
      ],
      "metadata": {
        "id": "eRVbaxncr4Mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is necessary to investigate data in order to gain insights and understand the underlying patterns and relationships in the data. This is important because the data that is used to train a machine learning model can significantly impact the model's performance. If the data is of poor quality or does not represent the problem well, the model is likely to perform poorly.\n",
        "\n",
        "There is a discrepancy in how qualitative and quantitative data are explored. Qualitative data is data that is descriptive and does not have a numerical value, such as text or categorical data. Quantitative data is data that has a numerical value and can be measured, such as numeric or ordinal data.\n",
        "\n",
        "Qualitative data is typically analyzed using techniques such as content analysis, where the data is coded and analyzed for themes and patterns. Quantitative data, on the other hand, is typically analyzed using statistical techniques, such as mean, median, and standard deviation.\n",
        "\n",
        "Regardless of whether the data is qualitative or quantitative, it is important to carefully examine and understand the data before using it to train a machine learning model. This will help ensure that the model is able to learn from the data and make accurate predictions."
      ],
      "metadata": {
        "id": "YiidvXv7sBdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. How do we deal with data outliers?**"
      ],
      "metadata": {
        "id": "62tpKtFAueW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several ways to deal with data outliers:\n",
        "\n",
        "Ignore the outlier: This approach is usually taken when the outlier is not significant or does not have a large impact on the results.\n",
        "\n",
        "Trim the outlier: This approach involves removing the outlier from the dataset. This can be done by setting a threshold and removing all data points that are above or below the threshold.\n",
        "\n",
        "Transform the data: This approach involves applying a transformation to the data to make it more symmetrical and reduce the impact of the outlier. For example, **applying a log transformation to the data can make it more symmetrical and reduce the impact of outliers.**\n",
        "\n",
        "Impute the outlier: This approach involves replacing the outlier with a more reasonable value, such as the median or mean of the data.\n",
        "\n",
        "#**Use robust algorithms:**\n",
        "\n",
        " Some machine learning algorithms are more resistant to the effects of outliers than others. Using algorithms that are more robust to outliers can help reduce their impact on the results.\n",
        "Decision trees: Decision trees are a type of non-parametric model that is not sensitive to the scale of the data or the presence of outliers.\n",
        "\n",
        "Random forests: Random forests are an ensemble learning method that combines the predictions of multiple decision trees. They are also not sensitive to the scale of the data or the presence of outliers.\n",
        "\n",
        "Support vector machines (SVMs): SVMs are a type of linear model that is able to handle outliers well by using the support vectors (i.e., the data points that are closest to the decision boundary) to define the decision boundary.\n",
        "\n",
        "k-nearest neighbors (k-NN): The k-NN algorithm is a type of instance-based learning algorithm that is not sensitive to the presence of outliers. It makes predictions based on the average of the k-nearest neighbors, which helps to mitigate the impact of outliers.\n",
        "\n",
        "Median absolute deviation (MAD): MAD is a statistical measure that is used to identify outliers in data. It is based on the median of the data, rather than the mean, which makes it more robust to the presence of outliers.\n",
        "\n",
        "Ultimately, the best approach to dealing with outliers will depend on the specific characteristics of the data and the problem you are trying to solve. It may be necessary to try several different approaches and see which one works best."
      ],
      "metadata": {
        "id": "KlxwyZGYus_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. What are the various central inclination measures? Why does mean vary too much from median in\n",
        "certain data sets?**"
      ],
      "metadata": {
        "id": "exTL-_scyO5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Central inclination measures are statistical measures that are used to summarize the central tendency or average of a dataset. The most common central inclination measures are the mean, median, and mode.\n",
        "\n",
        "The mean is the arithmetic average of a dataset, and is calculated by summing all of the data points and dividing by the total number of points. The median is the middle value in a dataset when the values are sorted in ascending order. The mode is the value that occurs most frequently in the dataset.\n",
        "\n",
        "The mean can vary too much from the median in certain data sets due to the presence of outliers. Outliers are data points that are significantly different from the rest of the data, and they can have a large impact on the mean. For example, if a dataset has a few extremely large or small values, the mean will be pulled in the direction of the outliers, while the median will be less affected.\n",
        "\n",
        "In general, the mean is more sensitive to outliers than the median, so the median is often preferred when the data is skewed or has outliers. However, the mean is still a useful measure of central tendency and can be more accurate in some cases, such as when the data is normally distributed."
      ],
      "metadata": {
        "id": "qc-9LRWgyTeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a difference in the formula for the variance of a sample and the variance of a population because the two formulas are used to calculate the variance of different types of data.\n",
        "\n",
        "The variance of a sample is used to estimate the variance of a population based on a sample of data from the population. The formula for the variance of a sample is:\n",
        "\n",
        "variance = ∑(x - mean)^2 / (n - 1)\n",
        "\n",
        "where x is a data point, mean is the mean of the sample, and n is the size of the sample.\n",
        "\n",
        "The variance of a population is the actual variance of the population, calculated using all of the data points in the population. The formula for the variance of a population is:\n",
        "\n",
        "variance = ∑(x - mean)^2 / n\n",
        "\n",
        "where x is a data point, mean is the mean of the population, and n is the size of the population.\n",
        "\n",
        "*The main difference between the two formulas is the denominator, which is n-1 for the sample variance and n for the population variance. The denominator is used to correct for bias in the estimate of the variance, and the difference between the two formulas reflects the fact that the sample variance is an estimate of the population variance and is therefore subject to additional error.*"
      ],
      "metadata": {
        "id": "Lj9BhvGOzmHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.Describe how a scatter plot can be used to investigate bivariate relationships. Is it possible to find\n",
        "outliers using a scatter plot?**\n",
        "\n"
      ],
      "metadata": {
        "id": "quY4YOZkzwH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot is a graphical representation of the relationship between two variables. It is a useful tool for investigating bivariate relationships, as it allows you to visualize the relationship between the two variables and see if there is any apparent trend or pattern.\n",
        "\n",
        "To create a scatter plot, you plot each data point as a pair of (x, y) coordinates, with the x-axis representing one variable and the y-axis representing the other variable. If there is a positive relationship between the two variables, the points on the scatter plot will tend to fall along a line that slopes upwards from left to right. If there is a negative relationship between the two variables, the points on the scatter plot will tend to fall along a line that slopes downwards from left to right.\n",
        "\n",
        "It is possible to find outliers using a scatter plot by looking for points that fall significantly outside the trend or pattern of the other points. Outliers are data points that are significantly different from the rest of the data, and they can have a large impact on statistical measures such as the mean and variance."
      ],
      "metadata": {
        "id": "8cEhTV4h0BiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Describe how cross-tabs can be used to figure out how two variables are related.**"
      ],
      "metadata": {
        "id": "_jVaVeHVz4re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Cross-tabs, also known as contingency tables or crosstabs, are a tool for analyzing the relationship between two categorical variables.* \n",
        "\n",
        "A cross-tab presents the frequency distribution of the variables in a table, with the rows representing one variable and the columns representing the other variable.\n",
        "\n",
        "By examining the frequencies in the table, you can determine how the two variables are related. For example, if one variable is significantly higher for a particular category of the other variable, this suggests that there is a relationship between the two variables. Cross-tabs can be used to identify patterns and trends in the data and to gain insights into the relationship between the two variables."
      ],
      "metadata": {
        "id": "blgi8hGh0YCj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L2dNi97gz7JA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}