{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. In a linear equation, what is the difference between a dependent variable and an independent\n",
        "variable?**\n"
      ],
      "metadata": {
        "id": "RzZoOlF9ZBj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a linear equation, the dependent variable is the variable that is being predicted or explained by the independent variable. It is also known as the response variable. The independent variable, also known as the predictor variable, is the variable that is used to make predictions or explain the changes in the dependent variable.\n",
        "For example, in the equation y = mx + b, y is the dependent variable, and x is the independent variable."
      ],
      "metadata": {
        "id": "L8FqM0odZQoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**2. What is the concept of simple linear regression? Give a specific example.**\n"
      ],
      "metadata": {
        "id": "IL4lSZq7ZGTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple linear regression is a statistical method that is used to study the relationship between two variables. It is used to model the relationship between a dependent variable and an independent variable. In simple linear regression, the relationship between the variables is modeled as a linear equation, where the dependent variable is the response variable and the independent variable is the predictor variable.\n",
        "For example, if we want to study the relationship between the number of hours studied and the exam score, we can use simple linear regression. In this case, the number of hours studied would be the independent variable, and the exam score would be the dependent variable. The linear equation would be:\n",
        "exam score = m * (number of hours studied) + b"
      ],
      "metadata": {
        "id": "Y3BaLkLRZVfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. In a linear regression, define the slope.**"
      ],
      "metadata": {
        "id": "t8WFX7FdZKQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a linear regression, the slope (m) is the coefficient of the independent variable. It represents the rate of change of the dependent variable for a unit change in the independent variable. In other words, it represents how much the dependent variable changes for a unit change in the independent variable. The slope can be positive, negative, or zero, and it determines the direction of the line. A positive slope means that the line is increasing, a negative slope means that the line is decreasing"
      ],
      "metadata": {
        "id": "QDUN9foDZa9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. What is multiple linear regression and how does it work?**\n",
        "\n"
      ],
      "metadata": {
        "id": "Oxh5Wfm0ZiyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple linear regression is a statistical method that is used to study the relationship between multiple independent variables and a single dependent variable. It is an extension of simple linear regression, where one predictor variable is used, to multiple predictor variables. It allows us to model the relationship between the dependent variable and multiple independent variables simultaneously.\n",
        "In multiple linear regression, the relationship between the variables is modeled as a linear equation, where the dependent variable is the response variable, and the independent variables are the predictor variables. The linear equation takes the form:\n",
        "\n",
        "y = b0 + b1x1 + b2x2 + ... + bn*xn\n",
        "\n",
        "where y is the dependent variable, x1, x2, ..., xn are the independent variables, b0 is the y-intercept, and b1, b2, ..., bn are the coefficients of the independent variables."
      ],
      "metadata": {
        "id": "WihbKv2JZyOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. In multiple linear regression, define the number of squares due to error.**"
      ],
      "metadata": {
        "id": "IVQZDQNYZmzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In multiple linear regression, the total sum of squares due to error (SSE) is the sum of the squared differences between the observed values of the dependent variable and the predicted values of the dependent variable. It represents the amount of variation in the dependent variable that is not explained by the independent variables."
      ],
      "metadata": {
        "id": "QZ_CtIUgZ14x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. In multiple linear regression, define the number of squares due to regression.**"
      ],
      "metadata": {
        "id": "y7L6aviyZsaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In multiple linear regression, the total sum of squares due to regression (SSR) is the sum of the squared differences between the predicted values of the dependent variable and the mean of the dependent variable. It represents the amount of variation in the dependent variable that is explained by the independent variables. The SSR is also known as explained sum of squares (ESS) and it is the sum of the squares of the differences between the predicted values of the dependent variable and the mean value of the dependent variable.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y4OeGGxfZ5Rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. What is heteroskedasticity, and what does it mean?**"
      ],
      "metadata": {
        "id": "fQPITeb-aFe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heteroskedasticity refers to a situation in which the variance of the error term in a linear regression model is not constant across all levels of the independent variables. It occurs when the spread of the residuals is not the same for all values of the independent variables. This means that the error term has a different variance for different levels of the independent variable. This problem can lead to incorrect estimation of the standard errors and confidence intervals of the regression coefficients, which can lead to incorrect conclusions about the relationships between the variables."
      ],
      "metadata": {
        "id": "m0OLEdm4alTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Describe the concept of ridge regression.**\n"
      ],
      "metadata": {
        "id": "aH9fx-1XaJUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge regression is a variation of linear regression that is used to address the problem of multicollinearity, which occurs when the independent variables in a linear regression model are highly correlated. **Ridge regression adds a shrinkage parameter, called the regularization parameter lambda, to the linear regression equation. This regularization parameter lambda is used to reduce the magnitude of the regression coefficients by adding a penalty term to the linear regression equation.** The result is a set of regression coefficients that are less sensitive to the collinearity of the independent variables."
      ],
      "metadata": {
        "id": "1sArwZcuao3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. Describe the concept of lasso regression.**"
      ],
      "metadata": {
        "id": "zVLcDMHFaPLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso regression is a variation of linear regression that is used to address the problem of multicollinearity and overfitting. Like Ridge regression, Lasso regression also adds a shrinkage parameter, called the regularization parameter lambda, to the linear regression equation, but it uses a different penalty term. **The Lasso penalty term is the absolute value of the regression coefficients, multiplied by lambda. This penalty term has the effect of shrinking the regression coefficients of less important variables towards zero, effectively reducing their contribution to the model, and eventually eliminating them.** This is called variable selection, and the Lasso approach is often used as a variable selection method."
      ],
      "metadata": {
        "id": "GUkYIITPaswb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14. What is polynomial regression and how does it work?**\n"
      ],
      "metadata": {
        "id": "YEexsmyGbaTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is a type of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial. Polynomial regression extends linear regression by adding extra polynomial terms to the equation. It is useful for modeling non-linear relationships between variables, and it can be used for both linear and non-linear regression problems.\n",
        "The equation for polynomial regression is:\n",
        "y = b0 + b1x + b2x^2 + ... + bn*x^n\n",
        "\n",
        "where y is the dependent variable, x is the independent variable, b0, b1, b2, ..., bn are the coefficients of the polynomial terms and n is the degree of the polynomial."
      ],
      "metadata": {
        "id": "g9Tapt-Lbpc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. Describe the basis function.**\n"
      ],
      "metadata": {
        "id": "ODhhKkUZbeg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A basis function is a function that is used to transform a variable into a new feature space. In polynomial regression, the basis function is the polynomial function, which transforms the independent variable x into a new feature space by adding polynomial terms to the linear regression equation. The polynomial terms are the basis functions, and they are used to fit a polynomial curve to the data."
      ],
      "metadata": {
        "id": "KBYRw8QDbsq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16. Describe how logistic regression works.**"
      ],
      "metadata": {
        "id": "eT7wBMpLbi3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a statistical method that is used for classification problems, where the goal is to predict a binary outcome (e.g., success or failure, yes or no). It is a variation of linear regression that is used to model the relationship between a binary dependent variable and one or more independent variables.\n",
        "\n",
        "In logistic regression, the relationship between the variables is modeled using a logistic function, which is a non-linear function that maps the input to a probability value between 0 and 1. The logistic function is used to estimate the probability that a given input belongs to a particular class. The predicted probability is then compared to a threshold value, usually 0.5, to make the final prediction."
      ],
      "metadata": {
        "id": "LCDO-duobxZi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPGxpKMSYVPW"
      },
      "outputs": [],
      "source": []
    }
  ]
}