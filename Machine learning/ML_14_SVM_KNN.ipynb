{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**7. In SVM, what is the cost of misclassification?**\n"
      ],
      "metadata": {
        "id": "-B5FIZRRrTSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In SVM, the cost of misclassification refers to the penalty assigned to misclassified data points. The cost of misclassification is controlled by a parameter called C, which is a trade-off between maximizing the margin and minimizing the misclassification errors. A smaller value of C results in a larger margin and fewer misclassification errors, while a larger value of C results in a smaller margin and more misclassification errors."
      ],
      "metadata": {
        "id": "MugK8ce3roWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. In the SVM model, define Support Vectors.**\n"
      ],
      "metadata": {
        "id": "qjHhAoB2rZ7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the SVM model, support vectors are the data points that are closest to the decision boundary. These data points are the ones that are used to define the decision boundary and are the most critical in determining the model's accuracy."
      ],
      "metadata": {
        "id": "5TnJnVR-rryI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. In the SVM model, define the kernel.**"
      ],
      "metadata": {
        "id": "PTfjQny6reXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the SVM model, the **kernel is a function that maps the input data into a higher-dimensional space, where it may be linearly separable. The kernel function allows SVM to create non-linear decision boundaries, which makes it suitable for datasets with complex, non-linear relationships.** Some commonly used kernel functions include the linear kernel, polynomial kernel, and radial basis function (RBF) kernel."
      ],
      "metadata": {
        "id": "homLGtA-rvNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. What are the factors that influence SVM&#39;s effectiveness?**"
      ],
      "metadata": {
        "id": "ToY2p_m8rijQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several factors that influence the effectiveness of an SVM model, including:\n",
        "\n",
        "The choice of kernel function: The kernel function plays a critical role in determining the model's accuracy. Choosing the right kernel function can improve the model's performance, while choosing the wrong one can lead to poor results.\n",
        "\n",
        "The value of the regularization parameter: The regularization parameter controls the trade-off between maximizing the margin and minimizing the misclassification errors."
      ],
      "metadata": {
        "id": "DjdrmpRdrzUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. What are the benefits of using the SVM model?**"
      ],
      "metadata": {
        "id": "e0jrIK46qp9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Support Vector Machine (SVM) model is a powerful and widely used machine learning algorithm that has several benefits, including:\n",
        "\n",
        "Robustness to outliers: SVM is less sensitive to outliers than other algorithms such as logistic regression, which makes it suitable for datasets with noisy or inconsistent data.\n",
        "\n",
        "High dimensional feature spaces: SVM can handle high dimensional feature spaces, which means it can work well with datasets that have a large number of features.\n",
        "\n",
        "Non-linear decision boundaries: SVM can create non-linear decision boundaries by using kernel functions, which makes it suitable for datasets with complex, non-linear relationships.\n",
        "\n",
        "Versatility: SVM can be used for both classification and regression problems, which makes it a versatile algorithm.\n",
        "\n",
        "Handling unbalanced data: SVM can handle unbalanced data by adjusting the misclassification error of different classes."
      ],
      "metadata": {
        "id": "6c4LV1UlrFk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. What are the drawbacks of using the SVM model?**"
      ],
      "metadata": {
        "id": "vaEgYxfzqyGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite its benefits, SVM also has some drawbacks, including:\n",
        "\n",
        "Computational cost: SVM can be computationally expensive, especially for large datasets with a high number of features.\n",
        "\n",
        "Hyperparameter tuning: SVM requires careful tuning of the kernel function and regularization parameter, which can be time-consuming and difficult for users with little or no background in statistics or machine learning.\n",
        "\n",
        "Lack of interpretability: SVM models can be difficult to interpret, which can make it"
      ],
      "metadata": {
        "id": "FxH7v6A3q-5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. Notes should be written on**\n",
        "\n",
        "1. The kNN algorithm has a validation flaw.\n",
        "\n",
        "3. A decision tree with inductive bias"
      ],
      "metadata": {
        "id": "rLbQfDmGnBLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The kNN algorithm has a validation flaw, **this refers to the fact that kNN algorithm doesn't work well with large datasets or high dimensional datasets, because the complexity of the algorithm increases with the size of the dataset and dimensionality.** Therefore, large datasets may lead to a high computational cost and longer processing time. Additionally, it can also lead to overfitting, which occurs when the algorithm is able to memorize the training data but is not able to generalize well to new data.\n",
        "\n",
        "A decision tree with **inductive bias refers to the fact that the decision tree algorithm has built-in assumptions about the underlying structure of the data. This can lead to the algorithm making predictions that are not accurate for the entire dataset, but only for the subset of data that was used to train the model.** This means that the decision tree algorithm may work well on the training data but not generalize well to new data. Additionally, the decision tree algorithm may be sensitive to the selection of the initial conditions and the parameters used. Therefore, it is important to be mindful of the inductive bias when using decision tree and to carefully evaluate the performance of the model on both the training and testing data."
      ],
      "metadata": {
        "id": "kW31BtllnMwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**17. What is the difference between a node and a leaf in a decision tree?**\n"
      ],
      "metadata": {
        "id": "IlILSv1VmCa1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a decision tree, a node represents a point in the tree where a decision or split is made, while a leaf represents a point in the tree where a prediction or classification is made. A node can have one or more child nodes, which are branches that lead to other decisions or splits. A leaf, on the other hand, does not have any child nodes and is the end point of a branch, where the final prediction or classification is made."
      ],
      "metadata": {
        "id": "syk5ah0GmRlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**18. What is a decision tree&#39;s entropy?**\n"
      ],
      "metadata": {
        "id": "wk0pduMTmGzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a decision tree, entropy is a measure of impurity or disorder of a set of data. It is used to determine the quality of a split at a node in the decision tree. \n",
        "\n",
        "Entropy is calculated using the formula:\n",
        "\n",
        "Entropy = -p(i)log2(p(i))\n",
        "\n",
        "Where p(i) is the proportion of data points in a particular class i. The entropy value ranges from 0 to 1, where 0 represents a pure dataset and 1 represents a completely mixed dataset. The goal of the decision tree algorithm is to select the split that results in the lowest entropy.\n",
        "\n"
      ],
      "metadata": {
        "id": "TC2sKbBtmUzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19. In a decision tree, define knowledge gain.**"
      ],
      "metadata": {
        "id": "srMGTb_NmLmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a decision tree, knowledge gain is a measure of the decrease in impurity or disorder of a set of data after a split. It is used to determine the quality of a split at a node in the decision tree. Knowledge gain is calculated by subtracting the weighted average entropy of the child nodes from the entropy of the parent node. The higher the knowledge gain, the better the split is, as it results in a more homogeneous set of data in the child nodes."
      ],
      "metadata": {
        "id": "Pnpc-qF5mcQs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-VoZHualuZ-"
      },
      "outputs": [],
      "source": []
    }
  ]
}