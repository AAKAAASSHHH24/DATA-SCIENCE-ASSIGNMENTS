{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is your definition of clustering? What are a few clustering algorithms you might think of?**"
      ],
      "metadata": {
        "id": "aAa5P1l82nW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other groups (clusters). Some examples of clustering algorithms are:\n",
        "K-Means: it groups the data into k clusters, where k is a user-specified parameter.\n",
        "Hierarchical Clustering: it creates a tree-like structure called a dendrogram, where each leaf node represents a single data point and each non-leaf node represents a cluster of data points.\n",
        "DBSCAN: it groups data points that are close to each other and separates data points that are far away from the others\n",
        "Gaussian Mixture Model: it models each cluster as a Gaussian distribution"
      ],
      "metadata": {
        "id": "7lrv9pqC2sDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. What are some of the most popular clustering algorithm applications?**"
      ],
      "metadata": {
        "id": "0mk8aP0C2zwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some popular applications of clustering algorithms include:\n",
        "Market segmentation: grouping customers with similar characteristics to better target marketing efforts\n",
        "Image segmentation: grouping pixels in an image that have similar colors or textures\n",
        "Anomaly detection: identifying data points that do not belong to any cluster\n",
        "Data compression: reducing the dimensionality of the data by representing each data point with a cluster label instead of its original feature values."
      ],
      "metadata": {
        "id": "U-99GZ6D2wLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. When using K-Means, describe two strategies for selecting the appropriate number of clusters.**"
      ],
      "metadata": {
        "id": "N98Fj1bs237j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using K-Means, two strategies for selecting the appropriate number of clusters are:\n",
        "Elbow Method: it plots the explained variance against the number of clusters, the elbow point on the plot is considered as the number of clusters to use.\n",
        "Silhouette score: it measures how similar an object is to its own cluster compared to other clusters, a higher silhouette score indicates a better clustering."
      ],
      "metadata": {
        "id": "0W88z8lt27Wx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What is mark propagation and how does it work? Why would you do it, and how would you do it?**\n"
      ],
      "metadata": {
        "id": "re25x26435xW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mark Propagation is a method for clustering that propagates cluster labels from labeled data points to unlabeled data points. It works by starting with a small set of labeled data points and then propagating the cluster labels to nearby unlabeled data points. The process is repeated until all data points have been labeled. This method can be useful when the number of labeled data points is small or when it is difficult to identify the appropriate number of clusters.\n",
        "Why would you do it:\n",
        "\n",
        "It can be useful when you have a small labeled dataset\n",
        "It can be useful when it is difficult to determine the number of clusters\n",
        "How would you do it:\n",
        "\n",
        "Start with a small set of labeled data points\n",
        "Propagate the cluster labels to nearby unlabeled data points\n",
        "Repeat the process until all data points have been labeled"
      ],
      "metadata": {
        "id": "wKK5Syc94FKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Provide two examples of clustering algorithms that can handle large datasets. And two that look\n",
        "for high-density areas?**"
      ],
      "metadata": {
        "id": "85Y1r4NJ39vB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two examples of clustering algorithms that can handle large datasets are:\n",
        "Mini-batch K-Means: it is a variation of the K-Means algorithm that processes a small batch of data points at a time, which can reduce the memory requirements of the algorithm.\n",
        "Streaming K-Means: it is a variation of the K-Means algorithm that processes data points one at a time and updates the cluster centroids incrementally.\n",
        "Two examples of clustering algorithms that look for high-density areas are:\n",
        "\n",
        "DBSCAN: it groups data points that are close to each other and separates data points that are far away from the others\n",
        "Mean Shift: it looks for high-density areas by iteratively shifting the cluster centroids to the densest areas of the data."
      ],
      "metadata": {
        "id": "xDfEUpaz4IxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Can you think of a scenario in which constructive learning will be advantageous? How can you go\n",
        "about putting it into action?**\n"
      ],
      "metadata": {
        "id": "VJidQGoX4VSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constructive learning is a form of machine learning that constructs a model incrementally, one example at a time. One scenario in which constructive learning would be advantageous is when data is expensive to collect or when it is difficult to obtain a large dataset. This is because it allows you to start with a small dataset and build the model incrementally as more data becomes available.\n",
        "How can you go about putting it into action:\n",
        "\n",
        "Start with a small set of labeled data points\n",
        "Train a model on these data points\n",
        "As new data points become available, add them to the dataset and retrain the model\n",
        "Repeat this process until the desired performance is achieved"
      ],
      "metadata": {
        "id": "q0mtiTMv4f3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. How do you tell the difference between anomaly and novelty detection?**"
      ],
      "metadata": {
        "id": "MxcpYJIR4Z68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anomaly detection is the task of identifying data points that deviate significantly from the normal data points. Novelty detection is the task of identifying new and unseen data points that are different from the data points used to train the model. The main difference between the two is that anomaly detection assumes that there is a set of normal data points, while novelty detection assumes that all data points are new and unseen.\n",
        "Anomaly detection is used to identify abnormal behavior or events that do not conform to an established pattern or normal behavior, such as detecting fraud or a malfunctioning machine. Novelty detection, on the other hand, is used to identify new and unseen data points that are different from the data points used to train the model, such as detecting new types of cyber attacks or new customer segments.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RSRiVOE84i4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. What is a Gaussian mixture, and how does it work? What are some of the things you can do about\n",
        "it?**"
      ],
      "metadata": {
        "id": "wF1lXS8U4pdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Gaussian mixture is a probabilistic model that assumes that the underlying data is generated from a mixture of multiple Gaussian distributions, rather than a single Gaussian distribution. Each component of the mixture is a Gaussian distribution, which is defined by its mean and covariance. The Gaussian mixture model is a powerful tool for density estimation and can be used for tasks such as clustering, anomaly detection, and image segmentation.\n",
        "\n",
        "One thing you can do with a Gaussian mixture is to fit the model to a dataset, which involves estimating the parameters of the Gaussian distributions (means, covariances, and mixing coefficients) that best explain the data. Once the model is fitted, you can use it to predict the probability density of new data points, classify new data points based on the most likely component they belong to, or generate new samples from the model.\n",
        "\n",
        "Another thing you can do is to use the Expectation-Maximization (EM) algorithm to estimate the parameters of the model, which is a iterative algorithm to find the maximum likelihood estimates of the parameters.\n",
        "\n",
        "Additionally, you can use a variation called the Gaussian Mixture Model- Expectation Maximization (GMM-EM) to perform clustering, assuming that the data points are generated from a mixture of k Gaussian distributions."
      ],
      "metadata": {
        "id": "rjHbBzAc5F77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. When using a Gaussian mixture model, can you name two techniques for determining the correct\n",
        "number of clusters?**"
      ],
      "metadata": {
        "id": "8O8Py4Kj4tlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there are several techniques for determining the correct number of clusters when using a Gaussian mixture model, including:\n",
        "\n",
        "The elbow method: This technique is based on the idea that as the number of clusters increases, the average within-cluster distance (also known as the inertia or sum of squares) will decrease. The elbow method involves fitting the Gaussian mixture model for different values of k and plotting the average within-cluster distance as a function of k. The optimal number of clusters is chosen at the \"elbow\" point, where the decrease in the average within-cluster distance begins to level off.\n",
        "\n",
        "Bayesian Information Criterion (BIC) or the Akaike Information Criterion (AIC): These are model selection criteria that can be used to compare different models with different numbers of clusters. The BIC and AIC are based on the likelihood of the data given the model, but also penalize models with more parameters (i.e. more clusters). The optimal number of clusters is chosen as the one that minimizes the BIC or AIC value.\n",
        "\n",
        "It's worth noting that these are not the only techniques to determine the correct number of clusters, and it is also dependent on the problem at hand, the data and the use case."
      ],
      "metadata": {
        "id": "eQ02ALJR5WFy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRVmIlqn2g87"
      },
      "outputs": [],
      "source": []
    }
  ]
}