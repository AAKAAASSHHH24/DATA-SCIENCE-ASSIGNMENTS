{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**2. Mention a few unsupervised learning applications.**\n"
      ],
      "metadata": {
        "id": "X_Lh1FsL3Ua3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsupervised learning is a type of machine learning where the model is not provided with labeled data.\n",
        "\n",
        " Some examples of unsupervised learning applications are:\n",
        "\n",
        "**Clustering:** Grouping similar data points together\n",
        "\n",
        "**Dimensionality reduction:** reducing the number of features in the data while preserving important information\n",
        "\n",
        "Anomaly detection: Identifying unusual or abnormal data points\n",
        "\n",
        "Association rule learning: discovering interesting relationships between variables in large datasets"
      ],
      "metadata": {
        "id": "w2UytgcM6zwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What are the three main types of clustering methods? Briefly describe the characteristics of each.**"
      ],
      "metadata": {
        "id": "U98aztZu3Y3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The three main types of clustering methods are:\n",
        "\n",
        "**Centroid-based clustering:** In this method, each cluster is represented by a single central point, known as the centroid. This central point is calculated as the mean of all the points in the cluster. The popular examples of this method are K-Means, K-Medians and Fuzzy C-Means\n",
        "\n",
        "**Hierarchical clustering:** This method creates a tree-like structure, where **each non-leaf node is a cluster and leaf nodes are individual data points. The tree can be represented as a dendrogram.** The linkage between the clusters can be defined by different criteria, such as single linkage, complete linkage or average linkage.\n",
        "\n",
        "**Density-based clustering:** This method clusters based on the density of data points in the feature space. It creates clusters by connecting high-density regions that are separated by low-density regions. A popular example of this method is DBSCAN (Density-Based Spatial Clustering of Applications with Noise).\n",
        "\n",
        "All three methods have their own advantages and disadvantages and selection of method depends on the data and problem being solved."
      ],
      "metadata": {
        "id": "KF6rSBOG7AN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Explain how the k-means algorithm determines the consistency of clustering.**"
      ],
      "metadata": {
        "id": "_4qHKLVvSusb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The k-means algorithm uses an **objective function called the \"inertia\" or \"within-cluster sum of squares\" to determine the consistency of clustering. This function measures the sum of the squared distances between each point in a cluster and the cluster's centroid (i.e., the mean of all points in the cluster).** The k-means algorithm aims to minimize this function, so that the points in each cluster are as close as possible to the cluster's centroid, which in turn increases the consistency of the clustering. The algorithm stops when the inertia no longer decreases, meaning the clusters are consistent."
      ],
      "metadata": {
        "id": "jpp2nqhWS40D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. With a simple illustration, explain the key difference between the k-means and k-medoids\n",
        "algorithms.**"
      ],
      "metadata": {
        "id": "Key0oBo9TiZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-means and k-medoids are both clustering algorithms that aim to partition a set of data points into k clusters. However, there is a key difference in how they determine the center of each cluster.\n",
        "\n",
        "K-means uses the mean of the points in a cluster as the center, also called centroid. This is calculated as the average of all points in the cluster along each dimension.\n",
        "\n",
        "K-medoids on the other hand, uses actual data points as the center of the cluster. It selects one of the data points in the cluster to be the medoid, the point that minimizes the sum of the distances between it and all other points in the cluster.\n",
        "\n",
        "A simple illustration would be to imagine the two algorithms trying to cluster a set of points in a 2D plane. In k-means, the center of the cluster would be represented by a dot, while in k-medoids, the center of the cluster is one of the actual points in the cluster.\n",
        "\n",
        "In this way, k-medoids is considered more robust to noise and outliers than k-means because it doesn't rely on mean, which can be highly affected by extreme values."
      ],
      "metadata": {
        "id": "Hxnzyxw6TsVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. What is a dendrogram, and how does it work? Explain how to do it.**"
      ],
      "metadata": {
        "id": "6lTXaXmEVL7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dendrogram is a tree-like diagram that represents the hierarchical clustering of a set of data points. It is used to visualize the structure of a set of data points and the relationships among them. Each leaf node of the dendrogram represents a single data point, and each branch represents a cluster of points that have been grouped together based on their similarity.\n",
        "\n",
        "There are several algorithms to perform hierarchical clustering, but the most common one is called \"agglomerative hierarchical clustering\". It works by first treating each data point as its own cluster, then repeatedly merging the two closest clusters until all data points are in a single cluster.\n",
        "\n",
        "To create a dendrogram, the following steps are generally performed:\n",
        "\n",
        "Calculate the distance matrix of the data set, which will represent the similarity between each pair of data points.\n",
        "\n",
        "Starting with each data point as a single cluster, iteratively merge the closest two clusters based on their distance. This can be done using linkage methods such as single linkage, complete linkage, or average linkage.\n",
        "\n",
        "Plot the dendrogram by representing each cluster as a horizontal line, and linking each cluster to its parent cluster.\n",
        "\n",
        "Add the height of each linkage to the dendrogram, which is the distance between the merged clusters.\n",
        "\n",
        "Finally, cut the dendrogram into different clusters at a certain threshold of linkage distance\n",
        "\n",
        "It is also worth mentioning that there is a \"divisive\" hierarchical clustering where the entire data set is considered as a single cluster and it is recursively split into smaller clusters."
      ],
      "metadata": {
        "id": "LqGHpv_gVXqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. What exactly is SSE? What role does it play in the k-means algorithm?**"
      ],
      "metadata": {
        "id": "eonY2Fs2V2xU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SSE stands for \"Sum of Squared Errors\" and it is a measure of the difference between the data points and their corresponding cluster centroid in k-means algorithm.\n",
        "\n",
        "In k-means, the objective is to minimize the SSE, which is calculated as the sum of the squared distances between each data point and the centroid of the cluster it belongs to. The SSE is also known as the \"inertia\" of the clustering.\n",
        "\n",
        "The role of SSE in the k-means algorithm is to measure the quality of the clustering. A low SSE indicates that the clusters are tight, meaning that the data points in each cluster are close to the centroid, and therefore the clustering is consistent. A high SSE, on the other hand, indicates that the clusters are loose, meaning that the data points in each cluster are far from the centroid, and therefore the clustering is inconsistent.\n",
        "\n",
        "In the k-means algorithm, the SSE is used to stop the algorithm when the clusters are consistent. The algorithm stops when the SSE no longer decreases, meaning that the clusters have stabilized and cannot be further improved.\n",
        "\n",
        "Additionally, the SSE can be used as a measure of similarity between different clusters, it can be used to compare different results of clustering, finding the best k value or determining the optimal number of clusters in a dataset."
      ],
      "metadata": {
        "id": "vXfANl1qV7a-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. With a step-by-step algorithm, explain the k-means procedure.**"
      ],
      "metadata": {
        "id": "dfarolMeXvWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The k-means algorithm is a popular method for clustering data points into k clusters. The basic procedure is as follows:\n",
        "\n",
        "**Specify the number of clusters k:** Before running the algorithm, the user must specify the number of clusters that they want to divide the data into.\n",
        "\n",
        "**Initialize k centroids:** k cluster centers are chosen randomly from the data set.\n",
        "\n",
        "**Assign each data point to the closest centroid:** Each data point is assigned to the cluster whose centroid is closest to it. This can be done by calculating the distance between the data point and each centroid using Euclidean distance.\n",
        "\n",
        "**Recalculate the centroid of each cluster**: The new centroid of each cluster is calculated as the mean of all data points in that cluster.\n",
        "\n",
        "Repeat steps 3 and 4 until the centroids no longer change: The algorithm iterates between steps 3 and 4 until the centroids no longer change, or a maximum number of iterations is reached.\n",
        "\n",
        "The final clusters are the partitions of data points around the final centroids: The final result of the k-means algorithm is k clusters, each represented by a centroid and a set of data points that are closest to it."
      ],
      "metadata": {
        "id": "gqBAhtkRZXVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. In the sense of hierarchical clustering, define the terms single link and complete link.**"
      ],
      "metadata": {
        "id": "7Dfzeod2Z2l1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In hierarchical clustering, the term \"linkage\" refers to the method used to determine the distance between two clusters. There are several linkage methods, but two of the most common are single linkage and complete linkage.\n",
        "\n",
        "Single linkage: This linkage method creates a cluster by connecting two clusters that have the closest pair of data points. In other words, it looks for the minimum distance between any two points, one in each cluster, and merges the clusters if that distance is below a certain threshold. This linkage method is also known as \"nearest neighbor linkage\" or \"single linkage clustering\". Single linkage is sensitive to outliers, and it is inclined to form long and thin clusters.\n",
        "\n",
        "Complete linkage: This linkage method creates a cluster by connecting two clusters that have the farthest pair of data points. In other words, it looks for the maximum distance between any two points, one in each cluster, and merges the clusters if that distance is below a certain threshold. This linkage method is also known as \"farthest neighbor linkage\" or \"complete linkage clustering\". Complete linkage is less sensitive to outliers than single linkage, and it is inclined to form compact clusters.\n",
        "\n",
        "Both linkage methods are used in hierarchical clustering to determine the distance between two clusters, but they have different behaviors. Single linkage tends to produce clusters with elongated shapes, while complete linkage tends to produce clusters with compact shapes."
      ],
      "metadata": {
        "id": "2MkFN49ZZ8J1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If9ILnbM3K1I"
      },
      "outputs": [],
      "source": []
    }
  ]
}