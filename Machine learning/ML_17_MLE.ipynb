{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**6. Use the formulas for a and b to explain ordinary least squares.**\n"
      ],
      "metadata": {
        "id": "Laq5yL4LxRas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ordinary least squares (OLS) algorithm is a method for estimating the parameters of a linear regression model. The parameters of the model are typically represented by the variables \"a\" and \"b\" which are the coefficients of the line equation y = a + bx.\n",
        "\n",
        "The OLS method estimates the values of \"a\" and \"b\" that minimize the sum of the squared differences between the observed y values and the predicted y values. Mathematically, this can be represented as:\n",
        "\n",
        "∑(y - (a + b*x))^2\n",
        "\n",
        "In order to find the values of \"a\" and \"b\" that minimize this sum of squared differences, we can take the partial derivatives of the SSE with respect to \"a\" and \"b\" and set them equal to zero. Then we can solve for \"a\" and \"b\" using these equations:\n",
        "\n",
        "a = (∑y - b*∑x) / N\n",
        "\n",
        "b = (∑xy - (1/N)(∑x)(∑y)) / (∑x^2 - (1/N)(∑x)^2)\n",
        "\n",
        "Where N is the total number of observations, ∑x and ∑y are the sums of the x and y values respectively, and ∑x^2 and ∑x*y are the sums of the squared x values and the product of x and y values respectively.\n",
        "\n",
        "These formulas for \"a\" and \"b\" are the OLS estimators for the true population parameters of the linear regression model. Once the optimal values of \"a\" and \"b\" have been estimated using the OLS algorithm, the linear regression model can be used to make predictions for new values of x."
      ],
      "metadata": {
        "id": "M11C3mP6zHE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Provide a step-by-step explanation of the OLS algorithm.**"
      ],
      "metadata": {
        "id": "KM7SoDrpxX1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ordinary least squares (OLS) algorithm is a method for estimating the parameters of a linear regression model. The steps for the OLS algorithm are as follows:\n",
        "\n",
        "Collect a dataset of (x, y) pairs, where x is the independent variable and y is the dependent variable.\n",
        "\n",
        "Establish a linear regression model of the form y = beta0 + beta1*x + e, where beta0 and beta1 are the parameters to be estimated, and e is the error term.\n",
        "\n",
        "Use the method of least squares to find the values of beta0 and beta1 that minimize the sum of the squared errors (SSE) between the predicted y values and the actual y values. Mathematically this can be written as\n",
        "∑(y - (beta0 + beta1*x))^2\n",
        "\n",
        "These estimates of beta0 and beta1 are the OLS estimators of the true population parameters.\n",
        "\n",
        "Once the optimal parameters have been estimated, the linear regression model can be used to make predictions for new values of x.\n",
        "\n",
        "The goodness of fit can be determined by using R-squared and adjusted R-squared\n",
        "\n",
        "The p-value of each variable can be used to determine the significance of variables in the model."
      ],
      "metadata": {
        "id": "99KTNq-7yKuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Describe the regression analysis assumptions and the BLUE principle.**\n"
      ],
      "metadata": {
        "id": "kRfvsZuFzTko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression analysis is a widely used statistical technique for modeling the relationship between a dependent variable and one or more independent variables. However, in order for the results of a regression analysis to be valid, certain assumptions must be met:\n",
        "\n",
        "**Linearity:** The relationship between the independent and dependent variables should be linear.\n",
        "\n",
        "**Independence:** The observations in the dataset should be independent of each other.\n",
        "\n",
        "**Homoscedasticity:** The variance of the errors should be constant across all levels of the independent variables.\n",
        "\n",
        "**Normality:** The errors should be normally distributed.\n",
        "\n",
        "No multicollinearity: The independent variables should not be highly correlated with each other.\n",
        "\n",
        "**The BLUE principle stands for Best Linear Unbiased Estimator, Which are estimators that are linear, unbiased, and have the smallest variance among all unbiased estimators.**"
      ],
      "metadata": {
        "id": "RojnN4Gnzfgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Describe two major issues with regression analysis.**"
      ],
      "metadata": {
        "id": "MJja5itfzXsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two major issues with regression analysis include:\n",
        "\n",
        "Non-linearity: If the relationship between the independent and dependent variables is non-linear, then a linear regression model will not be able to accurately capture the relationship.\n",
        "\n",
        "Outliers: Outliers can have a large impact on the regression line, causing the results to be skewed. Additionally, outliers can cause the regression line to be sensitive to small changes in the data.\n",
        "\n",
        "Another issue is that the model may not be generalizable, meaning that the model will not be able to make predictions beyond the range of the data that was used to create the model.\n",
        "Overfitting is another potential problem, which occurs when a model is trained with too many features, which leads to poor predictions on unseen data."
      ],
      "metadata": {
        "id": "Yie02ONbz96v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14. Provide a detailed explanation of logistic regression.**\n",
        "\n"
      ],
      "metadata": {
        "id": "tVY9Ujvx1Iri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a statistical technique used for modeling the relationship between a binary dependent variable and one or more independent variables. It is a type of generalized linear model (GLM) that is used for predicting a binary outcome (1 / 0, Yes / No, True / False) given a set of independent variables.\n",
        "The logistic regression model is represented by the following equation:\n",
        "\n",
        "p(y) = 1 / (1 + e^(-b0 - b1x1 - b2x2 - ... - bn*xn))\n",
        "\n",
        "Where p(y) is the probability of the outcome being 1, e is the base of the natural logarithm, and b0, b1, b2, ..., bn are the coefficients of the independent variables x1, x2, ..., xn.\n",
        "\n",
        "The coefficients of the independent variables are estimated by maximizing the likelihood of the observed data. The maximum likelihood estimates (MLEs) of the coefficients are the values that make the observed data most likely.\n",
        "\n",
        "Once the coefficients have been estimated, the logistic regression model can be used to make predictions for new data. The predicted probability can be converted into a binary outcome (1 or 0) by setting a threshold probability."
      ],
      "metadata": {
        "id": "0r8oVjpt1Zw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. What are the logistic regression assumptions?**"
      ],
      "metadata": {
        "id": "DbOB6wx81PK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression makes following assumptions:\n",
        "Linearity: The log-odds of the dependent variable must have a linear relationship with the independent variables.\n",
        "\n",
        "Independence: The observations in the dataset should be independent of each other.\n",
        "\n",
        "Binary outcome: The dependent variable should be binary (0 or 1, success or failure, etc.).\n",
        "\n",
        "Large sample size: The sample size should be large enough to ensure that the maximum likelihood estimates (MLEs) of the coefficients are reliable.\n",
        "\n",
        "No multicollinearity: The independent variables should not be highly correlated with each other.\n",
        "\n",
        "No outliers: Outliers can have a large impact on the estimates of the coefficients and can cause the model to be unreliable.\n",
        "\n",
        "No perfect separation: The independent variables should not be able to perfectly predict the outcome of the dependent variable.\n",
        "\n",
        "It is important to note that these assumptions should be checked before fitting a logistic regression model and if any of these assumptions are not met, the model may not be reliable."
      ],
      "metadata": {
        "id": "DQtINcDv1ehb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16. Go through the details of maximum likelihood estimation.**"
      ],
      "metadata": {
        "id": "CCTJawFt2CS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximum likelihood estimation (MLE) is a method for estimating the parameters of a statistical model. **The goal of MLE is to find the parameter values that make the observed data most likely, given the assumed probability distribution of the data.**\n",
        "\n",
        "The process of MLE consists of the following steps:\n",
        "\n",
        "Specify the probability distribution of the data: The first step is to specify the probability distribution of the data. This is typically done by assuming a specific probability distribution, such as normal, binomial, or Poisson, and defining the probability density function (pdf) or probability mass function (pmf) of the distribution.\n",
        "\n",
        "Define the likelihood function: The likelihood function is a function that describes the probability of the observed data, given a set of parameter values. The likelihood function is typically the product of the probability density function or probability mass function for each data point, evaluated at the observed value, given the assumed parameter values.\n",
        "\n",
        "Estimate the parameters: The next step is to estimate the parameters of the distribution by finding the values that maximize the likelihood function. This is typically done by taking the partial derivatives of the likelihood function with respect to each parameter and setting them equal to zero, and then solving for the parameter values.\n",
        "\n",
        "Check the validity of the model: The last step is to check the validity of the model by comparing the observed data and the estimated parameters, and evaluating the goodness of fit.\n",
        "\n",
        "Once the parameter values have been estimated, the model can be used to make predictions for new data.\n",
        "\n",
        "**It's important to note that MLE assumes that the sample data is independently and identically distributed and that the sample size is large, otherwise the estimates may not be reliable. Also, the maxima of likelihood function is not always unique and there could be multiple maxima.**"
      ],
      "metadata": {
        "id": "37evd6GS2atJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFw6qB5lxCW3"
      },
      "outputs": [],
      "source": []
    }
  ]
}