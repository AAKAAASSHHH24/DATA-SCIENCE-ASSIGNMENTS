{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**5. Describe the feature engineering process in the sense of a text categorization issue.**"
      ],
      "metadata": {
        "id": "0sS6QMND6wKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In text categorization, feature engineering is the process of extracting relevant features from the text data that can be used to train a machine learning model. The process typically involves the following steps:\n",
        "\n",
        "Text preprocessing: This includes cleaning the text data, removing stop words, punctuations, and converting the text to lowercase.\n",
        "\n",
        "Tokenization: This involves breaking down the text into individual words or phrases.\n",
        "\n",
        "Vectorization: This is the process of converting the text into numerical values that can be used as input for a machine learning model. Common techniques include bag-of-words and TF-IDF.\n",
        "\n",
        "Feature selection: This is the process of selecting the most relevant features from the vectorized data. This can be done using techniques such as chi-squared test or mutual information.\n",
        "\n",
        "Feature scaling: This step is applied to make sure that the features are in the same scale, which can improve the performance of the model. Common techniques include normalization and standardization.\n",
        "\n",
        "Training and evaluating the model on the selected features to make predictions.\n",
        "\n",
        "It is worth noting that feature engineering is an iterative process, and it is not uncommon to go back and repeat some of the steps in order to improve the performance of the model."
      ],
      "metadata": {
        "id": "7gi9kzcm7Hh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. What makes cosine similarity a good metric for text categorization? A document-term matrix has\n",
        "two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in\n",
        "cosine.**"
      ],
      "metadata": {
        "id": "jg1YDkbyHDrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine similarity is a good metric for text categorization because it is a measure of similarity between two vectors and is based on the angle between them. In the context of text categorization, vectors can be represented as document-term matrices, where each row represents a document and each column represents a term.\n",
        "\n",
        "The cosine similarity between two vectors is calculated as the dot product of the vectors divided by the product of their magnitudes. The dot product of two vectors is the sum of the products of the corresponding elements of the vectors, and the magnitude of a vector is the square root of the sum of the squares of its elements.\n",
        "\n",
        "To calculate the cosine similarity between the two document-term matrix rows you provided, you would need to perform the following steps:\n",
        "\n",
        "Calculate the dot product of the two vectors by multiplying the corresponding elements of the vectors and summing the products.\n",
        "\n",
        "Calculate the magnitude of each vector by taking the square root of the sum of the squares of the elements.\n",
        "\n",
        "Divide the dot product by the product of the magnitudes.\n",
        "\n",
        "The dot product is (22) + (31) + (20) + (00) + (23) + (32) + (31) + (03) + (11) = 18\n",
        "The magnitude of the first vector is sqrt((22) + (33) + (22) + (00) + (22) + (33) + (33) + (00) + (11)) = sqrt(43)\n",
        "The magnitude of the second vector is sqrt((22) + (11) + (00) + (00) + (33) + (22) + (11) + (33) + (1*1)) = sqrt(20)\n",
        "\n",
        "The Cosine Similarity between the two vectors is 18 / (sqrt(43) * sqrt(20)) = 0.68\n",
        "\n",
        "So the resemblance between these two vectors is 0.68 which means the vectors are similar but not identical.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bwf13vT-HApM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.\n",
        "\n",
        "**i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
        "calculate the Hamming gap.**\n",
        "\n",
        "**ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
        "0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).**"
      ],
      "metadata": {
        "id": "3bQiaKn5Jdrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i. The Hamming distance is a measure of the number of positions at which two strings of equal length differ. The formula for calculating the Hamming distance between two strings of binary digits is the number of positions at which the corresponding digits are different.\n",
        "\n",
        "For example, to calculate the Hamming distance between the binary strings 10001011 and 11001111, you would compare each digit in the same position of the two strings and count the number of positions where they differ.\n",
        "\n",
        "10001011\n",
        "\n",
        "11001111\n",
        "\n",
        "In this case, there are 3 positions where the digits differ, so the Hamming distance is 3.\n",
        "\n",
        "ii. Jaccard index and similarity matching coefficient are two similarity measures used in information retrieval and data mining.\n",
        "\n",
        "Jaccard index is defined as the ratio of the size of the intersection of two sets to the size of the union of the sets. It is a measure of the similarity between two sets, with a value of 1 indicating that the sets are identical and a value of 0 indicating that the sets have no elements in common.\n",
        "\n",
        "For example, to calculate the Jaccard index for the two sets of feature values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), you would count the number of positions where the two sets have the same value (i.e., where they are both 1), and divide this by the total number of positions where the two sets have a value of 1.\n",
        "\n",
        "The number of positions where the two sets are both 1 is 4, and the total number of positions where the two sets have a value of 1 is 6. Therefore, the Jaccard index is 4/6 = 2/3\n",
        "\n",
        "The Similarity Matching Coefficient is defined as the ratio of the size of the intersection of two sets to the minimum of the size of the two sets.\n",
        "\n",
        "The number of positions where the two sets are both 1 is 4, and the total number of positions of the first set is 8. Therefore, the Similarity Matching Coefficient is 4/8 = 0.5\n",
        "\n",
        "In this case, Jaccard index and Similarity Matching Coefficient are different, Jaccard index is showing 2/3 and Similarity Matching Coefficient is showing 0.5. It means that both are indicating different similarity between the two sets."
      ],
      "metadata": {
        "id": "wC62vrpvKBVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. State what is meant by &quot;high-dimensional data set&quot;? Could you offer a few real-life examples?\n",
        "What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
        "What can be done about it?**"
      ],
      "metadata": {
        "id": "RhHmiodkiHl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A high-dimensional data set refers to a dataset with a large number of features or variables. For example, a dataset with 100,000 features would be considered high-dimensional. Real-life examples of high-dimensional datasets include image recognition, where each pixel can be considered a feature, or text classification, where each word in a document can be considered a feature.\n",
        "\n",
        "**The main difficulty in using machine learning techniques on high-dimensional data is the curse of dimensionality, which states that as the number of dimensions increases, the amount of data needed to accurately model the problem also increases exponentially.** \n",
        "\n",
        "This can lead to overfitting, where a model performs well on the training data but poorly on unseen data.\n",
        "\n",
        "To address these difficulties, techniques such as dimensionality reduction, feature selection and regularization can be used to reduce the number of features in the dataset, while still retaining the important information. Additionally, techniques such as cross-validation and ensemble methods can be used to reduce overfitting."
      ],
      "metadata": {
        "id": "qbFESLeYiNoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Make a comparison between:**\n",
        "\n",
        "Sequential backward exclusion vs. sequential forward selection\n",
        "\n",
        "Function selection methods: filter vs. wrapper\n",
        "\n",
        "SMC vs. Jaccard coefficient"
      ],
      "metadata": {
        "id": "Tc2_K4vsmAed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential backward exclusion and sequential forward selection are both methods used in feature selection, which is the process of selecting a subset of features from a larger set to use in a model. Sequential backward exclusion starts with all features included and removes the least important feature at each step, while sequential forward selection starts with no features included and adds the most important feature at each step.\n",
        "\n",
        "In function selection, filter methods assess the relevance of each feature independently of the classifier and the wrapper methods assess the relevance of the feature by training a classifier with different feature subsets.\n",
        "\n",
        "SMC (Sorenson-Dice coefficient) and Jaccard coefficient are both similarity measures used to compare the similarity of two sets. SMC is a variation of Jaccard coefficient, where the similarity is computed as twice the ratio of the size of intersection to the size of the union of two sets. Jaccard coefficient is computed as the ratio of the size of the intersection to the size of the union of two sets."
      ],
      "metadata": {
        "id": "oWIajdI-miLN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMW3ZCbG6ViD"
      },
      "outputs": [],
      "source": []
    }
  ]
}