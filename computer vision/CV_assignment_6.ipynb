{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1.What is the difference between TRAINABLE and NON-TRAINABLE PARAMETERS?**"
      ],
      "metadata": {
        "id": "wIYMSI9KRM8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a machine learning model, the term \"trainable parameters\" refers to the variables of the model that can be optimized through the training process. These are the parameters that the model learns from the training data in order to make predictions. The model adjusts the values of the trainable parameters through the training process in order to minimize the loss function and improve the accuracy of the model.\n",
        "\n",
        "On the other hand, \"non-trainable parameters\" are variables of the model that are fixed and are not updated during training. These parameters are typically used to specify certain aspects of the model architecture or to control the behavior of the training process itself. For example, the number of layers in a neural network is a non-trainable parameter, while the weights and biases of the layers are trainable parameters."
      ],
      "metadata": {
        "id": "mqNRBXsnRUc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. In the CNN architecture, where does the DROPOUT LAYER go?**"
      ],
      "metadata": {
        "id": "TQDThQ1pTL3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a convolutional neural network (CNN), the dropout layer is typically inserted between the fully connected layers. The dropout layer randomly sets a fraction of the inputs to zero during training, which helps to prevent overfitting.\n",
        "\n",
        "Here is an example of a CNN architecture with a dropout layer inserted between the fully connected (FC) layers:"
      ],
      "metadata": {
        "id": "E5go725XS6yh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *input -> convolutional layer -> pooling layer -> convolutional layer -> pooling layer -> fully connected layer -> dropout layer -> fully connected layer -> output*\n"
      ],
      "metadata": {
        "id": "aHv_hOLzTBJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the dropout layer can be inserted at any point in the CNN where overfitting is a concern. It is common to see the dropout layer inserted after the final fully connected layer as well.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V59VnjfOTTvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.What is the optimal number of hidden layers to stack?**"
      ],
      "metadata": {
        "id": "EjRkQXqaT_BF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimal number of hidden layers to use in a neural network can vary depending on the complexity of the problem you are trying to solve. In general, deeper networks (networks with more hidden layers) can learn more complex relationships in the data and can achieve higher accuracy on the training set. However, deeper networks are also more prone to overfitting and may not generalize as well to unseen data.\n",
        "\n",
        "There is no one-size-fits-all answer to the question of how many hidden layers to use in a neural network. The best approach is to try different architectures and see which one performs the best on your specific problem.\n",
        "\n",
        "Here are a few things to consider when choosing the number of hidden layers:\n",
        "\n",
        "For simpler problems, a single hidden layer with a sufficient number of neurons may be sufficient.\n",
        "For more complex problems, deeper networks with more hidden layers may be necessary.\n",
        "If the training set is small, it is generally better to use fewer hidden layers to avoid overfitting.\n",
        "If the training set is large, deeper networks may be able to learn more complex relationships in the data and can achieve higher accuracy.\n",
        "\n",
        "In general, it is a good idea to start with a small number of hidden layers and increase the number gradually, keeping an eye on the performance of the model on the validation set. This can help you find the optimal number of hidden layers for your specific problem."
      ],
      "metadata": {
        "id": "prPRilBxUgvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. In each layer, how many secret units or filters should there be?**"
      ],
      "metadata": {
        "id": "Vva-oubrVpqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a neural network, the number of \"units\" or \"neurons\" in a layer refers to the number of nodes in that layer. The number of units or neurons in a layer is a hyperparameter that you can adjust to try to improve the performance of the model.\n",
        "\n",
        "The number of units or neurons in a layer is often referred to as the \"width\" of the layer. The \"depth\" of a neural network refers to the number of layers it has. A network with a large width and a small depth is called a \"wide\" network, while a network with a small width and a large depth is called a \"deep\" network.\n",
        "\n",
        "The number of units or neurons in a layer is typically chosen based on the complexity of the problem you are trying to solve and the size of the training set. In general, a larger number of units or neurons in a layer can allow the model to learn more complex relationships in the data, but it can also increase the risk of overfitting.\n",
        "\n",
        "In the case of convolutional neural networks (CNNs), the number of \"filters\" in a layer refers to the number of convolutional kernels that are applied to the input data in that layer. The number of filters in a layer is also a hyperparameter that you can adjust to try to improve the performance of the model. In general, a larger number of filters in a layer can allow the model to learn more complex features in the data, but it can also increase the risk of overfitting.\n",
        "\n",
        "As with the number of units or neurons in a layer, the number of filters in a layer is typically chosen based on the complexity of the problem and the size of the training set. It is a good idea to try different values for the number of filters and see which one performs the best on your specific problem."
      ],
      "metadata": {
        "id": "RpvNgYf9VwoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. What should your initial learning rate be?**"
      ],
      "metadata": {
        "id": "KG39aZVZV5Bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The learning rate is a hyperparameter that controls the step size at which the optimizer makes updates to the model parameters during training. The initial learning rate is the starting value for the learning rate that is used when training begins.\n",
        "\n",
        "The optimal learning rate can vary depending on the specific problem you are trying to solve and the characteristics of your model. If the learning rate is too small, training will progress very slowly, and it may take a long time to converge. If the learning rate is too large, the optimizer may overshoot the optimal solution and the training process may diverge.\n",
        "\n",
        "There is no one-size-fits-all answer to the question of what the initial learning rate should be. In general, it is a good idea to start with a moderate learning rate and then adjust it based on the performance of the model on the validation set.\n",
        "\n",
        "Here are a few things to consider when choosing the initial learning rate:\n",
        "\n",
        "For simpler problems, a smaller learning rate may be sufficient.\n",
        "For more complex problems, a larger learning rate may be necessary.\n",
        "If the training set is small, a smaller learning rate may be necessary to avoid overfitting.\n",
        "If the training set is large, a larger learning rate may be able to make more efficient use of the data and can lead to faster convergence.\n",
        "One way to choose the initial learning rate is to use a learning rate scheduler, which adjusts the learning rate over the course of training. There are many different learning rate schedulers available, and it is a good idea to try out a few different ones to see which one works best for your specific problem."
      ],
      "metadata": {
        "id": "_WzTrVXHWrd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. What do you do with the activation function?**"
      ],
      "metadata": {
        "id": "I2S6Bun8W_iH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The activation function in a neural network is a function that is applied to the output of a neuron. It determines the output of the neuron given a certain input or set of inputs. The activation function is an important part of a neural network because it allows the model to learn and represent non-linear relationships in the data.\n",
        "\n",
        "There are many different activation functions to choose from, and the choice of activation function can have a significant impact on the performance of the model. Some common activation functions include the sigmoid function, the tanh function, and the ReLU (rectified linear unit) function.\n",
        "\n",
        "In general, the activation function is chosen based on the characteristics of the problem you are trying to solve and the properties of the activation function. For example, the sigmoid function is often used in classification problems because it produces output values that are between 0 and 1, which can be interpreted as probabilities. The ReLU function is often used in deep learning models because it has been shown to work well in practice and can accelerate the training process.\n",
        "\n",
        "It is a good idea to try out different activation functions and see which one works best for your specific problem. In some cases, it may be beneficial to use multiple activation functions in a single model, with each function being used in different layers or for different purposes."
      ],
      "metadata": {
        "id": "netrs-nJX37Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. What is NORMALIZATION OF DATA?**"
      ],
      "metadata": {
        "id": "JoF3zV_GX884"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization is the process of scaling an input variable to a certain range. Normalization is often used in machine learning to scale the input variables so that they have comparable scales and to reduce the impact of noise in the data.\n",
        "\n",
        "There are many different ways to normalize data, and the appropriate method to use can depend on the specific characteristics of the data and the needs of the model. Some common methods of normalization include:\n",
        "\n",
        "Min-Max normalization: This method scales the data to a specific range, such as 0 to 1 or -1 to 1.\n",
        "\n",
        "Standardization: This method scales the data so that it has zero mean and unit variance.\n",
        "\n",
        "Z-score normalization: This method scales the data so that it has a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "It is a good idea to normalize the input data when training a machine learning model because it can help to improve the performance of the model and can make it easier for the optimizer to find the optimal solution. Normalization can also be useful when comparing the performance of different models, as it can help to ensure that the data is on a comparable scale."
      ],
      "metadata": {
        "id": "0_SkOqkoZa0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. What is IMAGE AUGMENTATION and how does it work?**"
      ],
      "metadata": {
        "id": "OS928RmraIcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image augmentation is a technique used to artificially increase the size of a training dataset by creating modified versions of images in the dataset. This is often used in machine learning when the original dataset is small, or when the model is prone to overfitting (i.e. it performs well on the training data but poorly on new, unseen data).\n",
        "\n",
        "There are many ways to augment images, but some common techniques include:\n",
        "\n",
        "Rotating the image\n",
        "Flipping the image horizontally or vertically\n",
        "Cropping the image\n",
        "Zooming in or out on the image\n",
        "Adding noise to the image\n",
        "Adjusting the brightness or contrast of the image\n",
        "By applying these transformations to the training images, the model can learn to recognize patterns and features that are invariant to these transformations. This can help the model generalize better to new, unseen data."
      ],
      "metadata": {
        "id": "mObCGu6xZ9s7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. What is DECLINE IN LEARNING RATE?**"
      ],
      "metadata": {
        "id": "OCEKFE5dcWow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, the learning rate is a hyperparameter that controls the step size at which the optimizer makes updates to the model parameters during training. A larger learning rate means that the optimizer will make larger updates to the model parameters, which can lead to faster convergence but also the risk of overshooting the optimal solution. A smaller learning rate means that the optimizer will make smaller updates, which can be more stable but may take longer to converge.\n",
        "\n",
        "One common technique to improve the performance of a model is to decay the learning rate over time. This means that the learning rate is reduced at regular intervals during training. This can be done using a fixed schedule, where the learning rate is reduced by a certain percentage at fixed intervals, or using a dynamic schedule, where the learning rate is reduced based on the performance of the model on the validation set.\n",
        "\n",
        "There are several reasons why decaying the learning rate can be beneficial:\n",
        "\n",
        "It can help the model converge to a better minimum or optimum.\n",
        "It can help prevent the model from overfitting by reducing the step size as training progresses.\n",
        "It can help the model escape from poor local minima or saddle points.\n",
        "There are many different strategies for decaying the learning rate, and the choice of strategy can depend on the specific characteristics of the model and the dataset. It is often useful to try a few different strategies and see which one works best for a given problem."
      ],
      "metadata": {
        "id": "bIX6yZkXdOq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What does EARLY STOPPING CRITERIA mean?"
      ],
      "metadata": {
        "id": "ByS79hmadYR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early stopping is a technique used to prevent overfitting in machine learning by interrupting the training process before the model has fully converged. It works by monitoring the performance of the model on a validation set and interrupting training when the performance on the validation set stops improving or begins to deteriorate.\n",
        "\n",
        "The criterion for deciding when to stop training is called the early stopping criterion. This can be based on a variety of metrics, such as the loss function, the accuracy, or some other metric of interest. The early stopping criterion is usually defined as a threshold or a change in the metric over a certain number of epochs. For example, the training process might be stopped if the loss on the validation set does not improve for 10 epochs, or if the accuracy on the validation set decreases by more than 0.1% over 5 epochs.\n",
        "\n",
        "Early stopping is often used in conjunction with other techniques to prevent overfitting, such as regularization or dropout. It can be a useful tool for finding the optimal number of epochs to train a model, and it can help ensure that the model generalizes well to new, unseen data."
      ],
      "metadata": {
        "id": "86pKUDcndjHl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeDac7SJQ50F"
      },
      "outputs": [],
      "source": []
    }
  ]
}