{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. After each stride-2 conv, why do we double the number of filters?**"
      ],
      "metadata": {
        "id": "gnf8jK0oqO0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of the number of filters to use in a convolutional neural network (CNN) and the stride of the convolutions is typically determined by the desired complexity and capacity of the network, as well as the size and characteristics of the input data.\n",
        "\n",
        "In general, increasing the number of filters in a convolutional layer can increase the capacity of the network to learn complex patterns in the data. This can be helpful for tasks that require the network to detect a large number of distinct features or patterns in the input data.\n",
        "\n",
        "On the other hand, increasing the number of filters can also increase the computational complexity of the network and may require more training data to prevent overfitting. It is important to carefully consider the trade-off between the capacity of the network and the risk of overfitting when deciding on the number of filters to use.\n",
        "\n",
        "The stride of the convolutions, on the other hand, determines the step size used to slide the filters over the input data. A stride of 2 will skip over every other entry in the input tensor when applying the filters, which can reduce the size of the output tensor by a factor of 2 in each spatial dimension (height and width). This can be useful for reducing the computational complexity of the network and preventing overfitting, but may also reduce the capacity of the network to learn fine-grained patterns in the data.\n",
        "\n",
        "In summary, the decision to double the number of filters after each stride-2 convolution in a CNN may depend on the specific requirements and goals of the task at hand, and the trade-off between the capacity and complexity of the network."
      ],
      "metadata": {
        "id": "fy5wuuvOqaVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Why do we use a larger kernel with MNIST (with simple cnn) in the first conv?**"
      ],
      "metadata": {
        "id": "QFjkyIchrJOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of the kernel size in a convolutional neural network (CNN) is typically determined by the desired complexity and capacity of the network, as well as the size and characteristics of the input data.\n",
        "\n",
        "In general, using a larger kernel in a convolutional layer can increase the receptive field of the layer, which is the region in the input data that each neuron in the layer is able to \"see\" or \"receive\" information from. A larger receptive field can allow the network to consider a wider context when processing the input data, which can be helpful for tasks that require the network to detect patterns or features that are spread out over a larger area of the input data.\n",
        "\n",
        "On the other hand, using a larger kernel can also increase the computational complexity of the network and may require more training data to prevent overfitting. It is important to carefully consider the trade-off between the capacity of the network and the risk of overfitting when deciding on the kernel size to use.\n",
        "\n",
        "The MNIST dataset consists of 28x28 grayscale images of handwritten digits, and the specific requirements and goals of the task may influence the decision to use a larger kernel in the first convolutional layer. However, it is not possible to accurately determine the rationale for this decision without more information about the specific CNN architecture and implementation being used."
      ],
      "metadata": {
        "id": "R5DnivCArB6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What data is saved by ActivationStats for each layer?**"
      ],
      "metadata": {
        "id": "UlvLhXQxrZiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ActivationStats is a tool for analyzing the activations of a neural network during training. It can be used to monitor the distribution of the activations and the change in the distribution over time, as well as to detect potential issues such as vanishing gradients or overfitting.\n",
        "\n",
        "ActivationStats typically saves the following data for each layer of the neural network:\n",
        "\n",
        "The mean and standard deviation of the activations: These statistics describe the distribution of the activations over the entire batch of input data.\n",
        "\n",
        "The minimum and maximum activation values: These values provide information about the range of the activations and can help to identify potential issues such as vanishing gradients or exploding gradients.\n",
        "\n",
        "The percentage of activations that are zero: This statistic can be used to detect potential issues such as dead neurons or vanishing gradients, which can occur when a large proportion of the activations are zero.\n",
        "\n",
        "The distribution of the activations over time: ActivationStats can track the evolution of the activation statistics over multiple training iterations, which can be helpful for detecting changes in the distribution of the activations and identifying potential issues.\n",
        "\n",
        "Overall, ActivationStats provides a range of data about the activations of a neural network that can be useful"
      ],
      "metadata": {
        "id": "QSPvEhChrt5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. How do we get a learner&#39;s callback after they&#39;ve completed training?**"
      ],
      "metadata": {
        "id": "EXJHdN1xsSI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a callback after a learner (such as a neural network) has completed training, you can use the following approaches:\n",
        "\n",
        "Use a callback function: Many deep learning frameworks provide a callback mechanism that allows you to define a function that will be called at specific points during training, such as at the end of each epoch or at the end of training. You can use this mechanism to specify a callback function that will be called after the learner has completed training, and perform any necessary actions in the callback function, such as saving the trained model or evaluating its performance on a validation dataset.\n",
        "\n",
        "Use a training loop: If you are implementing the training process manually using a loop, you can simply add the necessary actions after the loop has completed, such as saving the trained model or evaluating its performance.\n",
        "\n",
        "Use a training event: Some deep learning frameworks provide a mechanism for specifying training events, which are actions that are triggered by certain events during training, such as the end of training. You can use this mechanism to specify an event that will be triggered after the learner has completed training, and perform any necessary actions in the event handler.\n",
        "\n",
        "Overall, there are many ways to get a callback after a learner has completed training, and the specific approach will depend on the deep learning framework you are using and your specific requirements."
      ],
      "metadata": {
        "id": "-sYtILVcsQYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. What are the drawbacks of activations above zero?**"
      ],
      "metadata": {
        "id": "DK_4h3XutDR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several potential drawbacks of using activations that are consistently above zero in a neural network:\n",
        "\n",
        "Vanishing gradients: Activations that are consistently above zero can cause the gradients of the network to become very small, which can make it difficult for the network to learn. This is known as the vanishing gradients problem, and it can occur when the activations are consistently large or when the activation function saturates (e.g., reaches a maximum or minimum value).\n",
        "\n",
        "Overfitting: Activations that are consistently above zero can also lead to overfitting, which is when the network performs well on the training data but poorly on unseen data. This can occur because the network is able to memorize the training data rather than generalize to new examples.\n",
        "\n",
        "Slow training: Activations that are consistently above zero can also slow down the training process, because the gradients of the network may be very small, making it difficult for the optimizer to make updates to the weights.\n",
        "\n",
        "Overall, it is generally recommended to use activation functions that are able to produce a range of activation values, rather than consistently producing activations that are above zero. This can help to avoid the issues described above and allow the network to learn more effectively."
      ],
      "metadata": {
        "id": "sgmHlMaItjgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.Draw up the benefits and drawbacks of practicing in larger batches?**"
      ],
      "metadata": {
        "id": "C2qpnFyKtoDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a neural network using larger batches of data can have a number of benefits and drawbacks:\n",
        "\n",
        "Benefits of using larger batches:\n",
        "\n",
        "Increased parallelism: Training on larger batches allows the network to process more data in parallel, which can take advantage of hardware acceleration (such as using a GPU) and can speed up the training process.\n",
        "\n",
        "Reduced variance in the gradient estimates: Training on larger batches can reduce the variance in the estimates of the gradients of the weights, which can help to stabilize the training process and improve the generalization of the network.\n",
        "\n",
        "Improved optimization: Training on larger batches can also improve the optimization of the network, because the optimizer has a larger set of examples to work with and can make more accurate updates to the weights.\n",
        "\n",
        "Drawbacks of using larger batches:\n",
        "\n",
        "Increased memory requirements: Training on larger batches requires storing more data in memory, which can be a limitation if the batch size exceeds the available memory.\n",
        "\n",
        "Reduced generalization: In some cases, training on larger batches can result in reduced generalization, because the network may not see a diverse enough set of examples and may overfit to the training data.\n",
        "\n",
        "Increased optimization time: Training on larger batches can also increase the optimization time, because the optimizer may need to take more steps to converge to a good solution."
      ],
      "metadata": {
        "id": "oYfnLal5uRtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Why should we avoid starting training with a high learning rate?**"
      ],
      "metadata": {
        "id": "VbA1ltxtuYv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is generally recommended to avoid starting training with a high learning rate for a few reasons:\n",
        "\n",
        "High learning rates can cause the weights of the network to diverge, resulting in unstable or diverging behavior. This can occur because the updates to the weights may be too large, causing the weights to oscillate or grow unbounded.\n",
        "\n",
        "High learning rates can also cause the gradients of the weights to become very small, leading to the vanishing gradients problem. This can make it difficult for the network to learn, because the gradients of the weights may be too small to make effective updates to the weights.\n",
        "\n",
        "High learning rates can also cause the network to converge to a poor local minimum or saddle point, rather than the global minimum of the loss function. This can result in a suboptimal solution, and may require more training iterations to recover.\n",
        "\n",
        "Overall, it is generally recommended to start training with a learning rate that is small enough to allow the network to learn effectively, and then gradually increase the learning rate as training progresses. This can help to avoid the issues described above and allow the network to learn more effectively."
      ],
      "metadata": {
        "id": "pthkTH9Lut3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. What are the pros of studying with a high rate of learning?**"
      ],
      "metadata": {
        "id": "rv3b876BvCSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a few potential benefits to training a neural network with a high learning rate:\n",
        "\n",
        "Faster convergence: A high learning rate can cause the weights of the network to converge to a good solution faster, because the updates to the weights are larger and the optimization process can make progress more quickly. This can be especially beneficial when training on large datasets, where the optimization process may take a long time to converge.\n",
        "\n",
        "Improved optimization: A high learning rate can also improve the optimization of the network, because the optimizer is able to explore a wider range of the weight space and can potentially find a better solution.\n",
        "\n",
        "Reduced training time: Using a high learning rate can also reduce the overall training time, because the optimization process can make progress more quickly and may require fewer iterations to converge.\n",
        "\n",
        "Overall, the benefits of training with a high learning rate depend on the specific requirements and characteristics of the task and the dataset, and the trade-off between the learning rate and the stability of the optimization process. It is generally recommended to carefully tune the learning rate to find the best balance between convergence speed and optimization stability."
      ],
      "metadata": {
        "id": "2ge80CtwvYU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Why do we want to end the training with a low learning rate?**"
      ],
      "metadata": {
        "id": "Ph72A-grxNh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is generally recommended to end training with a low learning rate for a few reasons:\n",
        "\n",
        "Improved optimization: A low learning rate at the end of training can help to fine-tune the weights of the network and potentially improve the overall optimization of the network. This is because the optimizer is able to make small, precise updates to the weights, which can help to refine the solution and potentially find a better global minimum of the loss function.\n",
        "\n",
        "Reduced risk of overfitting: Using a low learning rate at the end of training can also reduce the risk of overfitting, because the updates to the weights are smaller and the network is less likely to overfit to the training data.\n",
        "\n",
        "Stable convergence: A low learning rate at the end of training can also help to stabilize the convergence of the optimization process and ensure that the network has converged to a good solution.\n",
        "\n",
        "Overall, ending training with a low learning rate can help to improve the optimization and generalization of the network, and reduce the risk of overfitting and unstable convergence. However, the specific learning rate to use at the end of training will depend on the specific requirements and characteristics of the task and the dataset, and may require some trial and error to find the best balance between optimization and stability."
      ],
      "metadata": {
        "id": "fvt-TfQ8xLyU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoOCYkX-qDlt"
      },
      "outputs": [],
      "source": []
    }
  ]
}