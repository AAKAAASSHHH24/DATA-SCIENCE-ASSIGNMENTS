{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What are the advantages of a CNN for image classification over a completely linked DNN?**"
      ],
      "metadata": {
        "id": "W80UcMep22ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several advantages of convolutional neural networks (CNNs) for image classification over fully connected deep neural networks (DNNs):\n",
        "\n",
        "Spatial hierarchies: CNNs can utilize spatial hierarchies of features, which are important for image classification tasks. This means that they can learn to detect lower-level features (such as edges and shapes) and use these features to build up to higher-level features (such as object parts and objects). This hierarchical structure allows CNNs to be more efficient and effective for image classification tasks compared to DNNs, which do not have this structure.\n",
        "\n",
        "Translation invariance: CNNs are also translation invariant, which means that they are not affected by the position of an object in the input image. This is important for image classification tasks because objects can appear at different positions in an image and a good classifier should be able to recognize the object regardless of its position. DNNs, on the other hand, are not translation invariant and may perform poorly if the position of the object in the input image is changed.\n",
        "\n",
        "Less data required: CNNs also require less data to achieve good performance on image classification tasks compared to DNNs. This is because CNNs can learn hierarchical representations of the input data, which allows them to generalize better to new examples.\n",
        "\n",
        "Fewer parameters: Finally, CNNs have fewer parameters compared to DNNs, which makes them faster to train and easier to optimize. This is because CNNs use shared weights and biases, whereas DNNs do not.\n",
        "\n",
        "Overall, CNNs are well-suited for image classification tasks because they can effectively learn spatial hierarchies of features, are translation invariant, require less data, and have fewer parameters compared to DNNs."
      ],
      "metadata": {
        "id": "PVSJiRph3LJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Consider a CNN with three convolutional layers, each of which has three kernels, a stride of two,\n",
        "and SAME padding. The bottom layer generates 100 function maps, the middle layer 200, and the\n",
        "top layer 400. RGB images with a size of 200 x 300 pixels are used as input. How many criteria does\n",
        "the CNN have in total? How much RAM would this network need when making a single instance\n",
        "prediction if we&#39;re using 32-bit floats? What if you were to practice on a batch of 50 images?**"
      ],
      "metadata": {
        "id": "uN0TgyHX3sJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of parameters:\n",
        "The bottom convolutional layer has 3 kernels of size 200 x 300 x 3, so it has 3 * 200 * 300 * 3 = 3,240,000 parameters.\n",
        "The middle convolutional layer has 3 kernels of size 100 x 150 x 100, so it has 3 * 100 * 150 * 100 = 4,500,000 parameters.\n",
        "The top convolutional layer has 3 kernels of size 50 x 75 x 200, so it has 3 * 50 * 75 * 200 = 3,000,000 parameters.\n",
        "In total, the CNN has 3,240,000 + 4,500,000 + 3,000,000 = 10,740,000 parameters.\n",
        "\n",
        "RAM usage for a single instance prediction:\n",
        "If we're using 32-bit floats, the CNN will need 10,740,000 * 32 / 8 = 43,176,000 bits of RAM to store the parameters.\n",
        "This is equivalent to 43,176,000 / 8 = 5,397,000 bytes, or approximately 5.4 MB of RAM.\n",
        "\n",
        "RAM usage for a batch of 50 images:\n",
        "If we're making a prediction on a batch of 50 images, the CNN will need an additional 5.4 MB * 50 = 270 MB of RAM to store the input data and the activations of the convolutional layers.\n",
        "Therefore, the total RAM usage for the CNN would be 5.4 MB + 270 MB = 275.4 MB."
      ],
      "metadata": {
        "id": "QVh6ZQaI3pj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What are five things you might do to fix the problem if your GPU runs out of memory while\n",
        "training a CNN?**"
      ],
      "metadata": {
        "id": "x1BsHBI0358J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are five things you might do to fix the problem if your GPU runs out of memory while training a CNN:\n",
        "\n",
        "Reduce the batch size: One way to reduce the GPU memory usage is to reduce the batch size. This will reduce the number of input images that need to be processed at each training step, which will in turn reduce the amount of memory required by the GPU.\n",
        "\n",
        "Use a smaller network: You can also try using a smaller network with fewer layers and fewer parameters. This will reduce the total number of parameters that need to be stored in the GPU memory, which may allow the network to fit within the available memory.\n",
        "\n",
        "Reduce the resolution of the input images: If you are using high-resolution images as input to the network, you may be able to reduce the memory usage by downsampling the images to a lower resolution.\n",
        "\n",
        "Use a model with parameter sharing: Some network architectures, such as fully convolutional networks, use parameter sharing, which means that the same set of weights is used for multiple parts of the input. This can significantly reduce the number of parameters in the network and the corresponding memory usage.\n",
        "\n",
        "Use a GPU with more memory: If the above options are not feasible or sufficient, you may need to use a GPU with more memory. This will allow you to train larger networks or use larger batch sizes without running out of memory."
      ],
      "metadata": {
        "id": "xz7HgQQf4LHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Why would you use a max pooling layer instead with a convolutional layer of the same stride?**"
      ],
      "metadata": {
        "id": "_EQgVy_X4Xuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several reasons why you might use a max pooling layer instead of a convolutional layer with the same stride:\n",
        "\n",
        "Computational efficiency: Max pooling is generally faster and more computationally efficient than convolutional layers with the same stride. This is because max pooling involves only a simple comparison operation, whereas convolutional layers involve complex matrix multiplications.\n",
        "\n",
        "Spatial invariance: Max pooling is also more spatially invariant than convolutional layers with the same stride. This means that max pooling is less sensitive to the position of features in the input, which can be useful for tasks such as image classification.\n",
        "\n",
        "Overfitting: Max pooling can also help to prevent overfitting in a deep neural network by reducing the number of parameters and the amount of computation required. This can be especially important when training a network on a small dataset.\n",
        "\n",
        "Overall, max pooling is a useful tool in the design of convolutional neural networks because it can improve computational efficiency, increase spatial invariance, and reduce overfitting."
      ],
      "metadata": {
        "id": "99lCS4b54ceG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. When would a local response normalization layer be useful?**"
      ],
      "metadata": {
        "id": "pgxd5sCf4jSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Local response normalization (LRN) is a preprocessing step that is often used in convolutional neural networks (CNNs) for image classification tasks. It normalizes the responses of the neurons in a local neighborhood within the same channel.\n",
        "\n",
        "LRN can be useful in several situations:\n",
        "\n",
        "Improving the performance of a CNN: Some studies have shown that LRN can improve the performance of a CNN on image classification tasks. This is because LRN can help to increase the selectivity of the neurons in the network, which can make the network more robust to variations in the input data.\n",
        "\n",
        "Reducing overfitting: LRN can also help to reduce overfitting in a CNN by normalizing the activations of the neurons in a local neighborhood. This can make the network more robust to variations in the input data and prevent the network from overfitting to the training data.\n",
        "\n",
        "Enhancing generalization: LRN can enhance the generalization ability of a CNN, which means that the network can better perform on unseen data.\n",
        "\n",
        "Overall, LRN can be useful in improving the performance, reducing overfitting, and enhancing generalization of a CNN on image classification tasks."
      ],
      "metadata": {
        "id": "pI75cNdn4vvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. In comparison to LeNet-5, what are the main innovations in AlexNet? What about GoogLeNet and\n",
        "ResNet&#39;s core innovations?**"
      ],
      "metadata": {
        "id": "-SOte3lx44VI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet-5 is a pioneering convolutional neural network (CNN) that was developed by Yann LeCun et al. in the 1990s. It consists of several convolutional and fully connected layers and was one of the first deep learning models to be applied to the task of image classification.\n",
        "\n",
        "AlexNet is a CNN that was developed by Alex Krizhevsky et al. in 2012 and won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) that year. In comparison to LeNet-5, the main innovations in AlexNet include:\n",
        "\n",
        "Larger scale: AlexNet was trained on a much larger dataset (over 1 million images) and used a larger network with more layers and more parameters compared to LeNet-5.\n",
        "\n",
        "Use of rectified linear units (ReLUs): AlexNet used ReLUs as the activation function in the hidden layers, which improved the training speed and performance of the network compared to using sigmoid or tanh activation functions.\n",
        "\n",
        "Use of dropout: AlexNet introduced the use of dropout as a regularization technique, which helped to prevent overfitting and improve the generalization ability of the network.\n",
        "\n",
        "GoogLeNet is a CNN that was developed by Christian Szegedy et al. in 2014 and won the ILSVRC that year. Its core innovations include:\n",
        "\n",
        "Use of inception modules: GoogLeNet introduced the use of inception modules, which are blocks of layers that are designed to efficiently compute a large number of different convolutional and pooling operations in parallel. This allows GoogLeNet to have a much deeper and more complex network architecture while still being efficient and effective.\n",
        "\n",
        "Use of auxiliary classifiers: GoogLeNet also introduced the use of auxiliary classifiers, which are additional classifiers that are added to the network and trained in parallel with the main classifier. These auxiliary classifiers can help to improve the performance of the network by providing additional supervision and regularization.\n",
        "\n",
        "ResNet is a CNN that was developed by Kaiming He et al. in 2015 and won the ILSVRC that year. Its core innovations include:\n",
        "\n",
        "Use of skip connections: ResNet introduced the use of skip connections, which are shortcut connections that skip one or more layers and directly connect the input of the network to the output. These skip connections allow the gradients to flow more easily through the network and can help to alleviate the vanishing gradients problem, which can occur in very deep networks.\n",
        "\n",
        "Very deep networks: ResNet also demonstrated the effectiveness of very deep networks (with over 100 layers) for image classification tasks. This was made possible by the use of skip connections, which allowed the network to learn more complex and abstract features while still being able to train efficiently."
      ],
      "metadata": {
        "id": "YLa5YsB547sA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. On MNIST, build your own CNN and strive to achieve the best possible accuracy.**"
      ],
      "metadata": {
        "id": "FZrKsY8m5he1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST (Modified National Institute of Standards and Technology) is a widely used dataset for image classification tasks that consists of 60,000 training images and 10,000 test images of handwritten digits (0 to 9).\n",
        "\n",
        "Here is an example of how you might build a CNN to achieve the best possible accuracy on MNIST:\n",
        "\n",
        "Preprocessing: You can start by preprocessing the input images by converting them to grayscale and normalizing the pixel values to be between 0 and 1. You can also consider applying data augmentation techniques, such as rotation and translation, to the training data to increase the robustness of the network.\n",
        "\n",
        "Network architecture: You can then design the network architecture of your CNN. A common choice for MNIST is to use a series of convolutional layers followed by max pooling layers and fully connected layers. You can experiment with different hyperparameters, such as the number of filters, kernel size, and stride, to find the best combination for your network.\n",
        "\n",
        "Activation function: You can use ReLU (rectified linear unit) as the activation function in the hidden layers of your CNN. ReLU is a popular choice because it is computationally efficient and can improve the training speed and performance of the network.\n",
        "\n",
        "Optimizer and loss function: You can use a stochastic gradient descent (SGD) optimizer with a cross-entropy loss function to train your CNN. You can experiment with different learning rates and momentum values to find the best combination for your network.\n",
        "\n",
        "Regularization: You can use techniques such as dropout and weight decay to regularize your CNN and prevent overfitting.\n",
        "\n",
        "Evaluation: You can evaluate the performance of your CNN on the test set and iterate on the design of the network and the hyperparameters until you achieve the best possible accuracy.\n",
        "\n",
        "Overall, building a successful CNN for MNIST involves a combination of preprocessing, network architecture design, choice of activation function, optimization and loss function, and regularization techniques. By experimenting with different combinations and finding the best fit for your data, you can achieve high accuracy on the MNIST dataset."
      ],
      "metadata": {
        "id": "WA6hJSLh58LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Using Inception v3 to classify broad images. a.\n",
        "Images of different animals can be downloaded. Load them in Python using the\n",
        "matplotlib.image.mpimg.imread() or scipy.misc.imread() functions, for example. Resize and/or crop\n",
        "them to 299 x 299 pixels, and make sure they only have three channels (RGB) and no transparency.\n",
        "The photos used to train the Inception model were preprocessed to have values ranging from -1.0 to\n",
        "1.0, so make sure yours do as well."
      ],
      "metadata": {
        "id": "KaNFgLZH6gLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use Inception v3 to classify broad images using Python, you can follow these steps:\n",
        "\n",
        "Download the images: You can download the images of different animals using the matplotlib.image.mpimg.imread() or scipy.misc.imread() functions, for example.\n",
        "\n",
        "Resize and crop the images: You can then resize and/or crop the images to have dimensions of 299 x 299 pixels. This is the input size that Inception v3 expects.\n",
        "\n",
        "Make sure the images have three channels: You should also make sure that the images only have three channels (RGB) and no transparency.\n",
        "\n",
        "Normalize the pixel values: The photos used to train Inception v3 were preprocessed to have values ranging from -1.0 to 1.0, so you should make sure to normalize the pixel values of your images in the same way.\n",
        "\n",
        "Load the Inception v3 model: You can then load the Inception v3 model using a library such as TensorFlow or Keras.\n",
        "\n",
        "Classify the images: Finally, you can use the Inception v3 model to classify the images by providing it with the preprocessed images as input and using the model's predict() method to generate predictions."
      ],
      "metadata": {
        "id": "apZdH_Zd6edq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10ZLUAQY1nMp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "\n",
        "# Load an image of a dog\n",
        "image = mpimg.imread('dog.jpg')\n",
        "\n",
        "# Resize and crop the image to 299 x 299 pixels\n",
        "image = tf.image.resize_with_crop_or_pad(image, 299, 299)\n",
        "\n",
        "# Make sure the image has three channels (RGB) and no transparency\n",
        "image = tf.image.grayscale_to_rgb(image)\n",
        "\n",
        "# Normalize the pixel values to be between -1.0 and 1.0\n",
        "image = (image / 127.5) - 1.0\n",
        "\n",
        "# Load the Inception v3 model\n",
        "model = tf.keras.applications.InceptionV3()\n",
        "\n",
        "# Use the model to classify the image\n",
        "prediction = model.predict(image)\n",
        "\n",
        "# Print the top 5 predictions\n",
        "print(np.argmax(prediction, axis=1)[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will load an image of a dog, resize and crop it to 299 x 299 pixels, make sure it has three channels and no transparency, normalize the pixel values, load the Inception v3 model, and use the model to classify the image. The top 5 predictions will then be printed.\n",
        "\n",
        "Note that this code assumes that you have a file named 'dog.jpg' in the current directory that contains an image of a dog. You can modify the code to classify other images by simply changing the file name and image."
      ],
      "metadata": {
        "id": "0rvWd9JP7Sri"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJQAps29653X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}